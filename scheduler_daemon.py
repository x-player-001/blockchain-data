#!/usr/bin/env python3
"""
å®šæ—¶ä»»åŠ¡å®ˆæŠ¤è¿›ç¨‹
è´Ÿè´£è¿è¡Œä¸¤ä¸ªå®šæ—¶ä»»åŠ¡ï¼š
1. æ¯10åˆ†é’Ÿçˆ¬å– DexScreener é¦–é¡µæ•°æ®ï¼ˆå¯é€‰ï¼‰
2. æ¯5åˆ†é’Ÿè¿è¡Œç›‘æ§æ›´æ–°ä»·æ ¼

ä½¿ç”¨æ–¹æ³•ï¼š
- å¯åŠ¨æ‰€æœ‰ä»»åŠ¡ï¼ˆçˆ¬è™«+ç›‘æ§ï¼‰ï¼špython scheduler_daemon.py
- åªå¯åŠ¨ç›‘æ§ä»»åŠ¡ï¼špython scheduler_daemon.py --monitor-only
- æŸ¥çœ‹å¸®åŠ©ï¼špython scheduler_daemon.py --help
"""

import asyncio
import logging
import signal
import sys
import os
import random
import argparse
from datetime import datetime, timedelta
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.interval import IntervalTrigger
from apscheduler.triggers.date import DateTrigger

from src.services.token_monitor_service import TokenMonitorService
from src.services.multi_chain_scraper import MultiChainScraper
from src.services.kline_service import KlineService

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/tmp/scheduler.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# å…¨å±€å˜é‡
scheduler = None
monitor_service = None
enable_scraper = True  # æ˜¯å¦å¯ç”¨çˆ¬è™«ä»»åŠ¡
use_undetected_chrome = os.getenv('USE_UNDETECTED_CHROME', 'false').lower() == 'true'


async def scrape_dexscreener_task():
    """
    çˆ¬å– DexScreener é¦–é¡µä»»åŠ¡ï¼ˆä»æ•°æ®åº“è¯»å–é…ç½®ï¼‰
    ä½¿ç”¨ cloudscraper æˆ– undetected-chromedriver çˆ¬å–å¤šé“¾æ•°æ®
    æ”¯æŒé‡è¯•æœºåˆ¶æé«˜æˆåŠŸç‡
    """
    from src.storage.models import ScrapeLog
    from src.storage.db_manager import DatabaseManager
    import uuid

    scraper = None
    monitor_service = None
    scrape_log_id = None
    db_manager = None

    try:
        logger.info("="*80)
        logger.info("å¼€å§‹çˆ¬å– DexScreener é¦–é¡µï¼ˆå¤šé“¾ï¼‰...")
        logger.info("="*80)

        # 1. ä»æ•°æ®åº“è¯»å–é…ç½®
        monitor_service = TokenMonitorService()
        config = await monitor_service.get_scraper_config()

        if not config:
            logger.error("æœªæ‰¾åˆ°çˆ¬è™«é…ç½®ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            config = {
                'enabled_chains': ['bsc', 'solana'],
                'count_per_chain': 100,
                'top_n_per_chain': 10,
                'use_undetected_chrome': use_undetected_chrome,
                'enabled': True,
                'scrape_interval_min': 9,
                'scrape_interval_max': 15
            }

        # 2. æ£€æŸ¥é…ç½®æ˜¯å¦å¯ç”¨
        if not config.get('enabled', True):
            logger.info("çˆ¬è™«é…ç½®å·²ç¦ç”¨ï¼Œè·³è¿‡æœ¬æ¬¡çˆ¬å–")
            schedule_next_scrape(config)
            return

        logger.info(f"é…ç½®ä¿¡æ¯: chains={config['enabled_chains']}, "
                   f"count_per_chain={config['count_per_chain']}, "
                   f"top_n_per_chain={config['top_n_per_chain']}, "
                   f"use_undetected_chrome={config['use_undetected_chrome']}")

        # 2.5 åˆ›å»º ScrapeLog è®°å½•ï¼ˆçŠ¶æ€ï¼šrunningï¼‰
        start_time = datetime.utcnow()
        db_manager = DatabaseManager()
        scrape_log_id = str(uuid.uuid4())

        async with db_manager.get_session() as session:
            scrape_log = ScrapeLog(
                id=scrape_log_id,
                started_at=start_time,
                status='running',
                chain=','.join(config.get('enabled_chains', [])),  # å¤šé“¾ç”¨é€—å·åˆ†éš”
                config_snapshot=config  # ä¿å­˜é…ç½®å¿«ç…§
            )
            session.add(scrape_log)
            await session.commit()

        logger.info(f"ğŸ“ å·²åˆ›å»ºæŠ“å–æ—¥å¿—è®°å½•: {scrape_log_id}")

        # 3. ä½¿ç”¨é…ç½®å‚æ•°çˆ¬å–
        scraper = MultiChainScraper()

        # çˆ¬å–å¹¶ä¿å­˜åˆ° potential_tokens è¡¨
        result = await scraper.scrape_and_save_multi_chain(
            chains=config['enabled_chains'],              # ä»é…ç½®è¯»å–é“¾åˆ—è¡¨
            count_per_chain=config['count_per_chain'],    # ä»é…ç½®è¯»å–æ¯æ¡é“¾çˆ¬å–æ€»æ•°
            top_n_per_chain=config['top_n_per_chain'],    # ä»é…ç½®è¯»å–æ¯æ¡é“¾å–å‰Nå
            use_undetected_chrome=config['use_undetected_chrome'],  # ä»é…ç½®è¯»å–çˆ¬å–æ–¹æ³•
            min_market_cap=config.get('min_market_cap'),  # ä»é…ç½®è¯»å–æœ€å°å¸‚å€¼
            min_liquidity=config.get('min_liquidity'),    # ä»é…ç½®è¯»å–æœ€å°æµåŠ¨æ€§
            max_token_age_days=config.get('max_token_age_days')  # ä»é…ç½®è¯»å–æœ€å¤§ä»£å¸å¹´é¾„
        )

        # 3.5 æ›´æ–° ScrapeLog è®°å½•ï¼ˆçŠ¶æ€ï¼šsuccessï¼‰
        end_time = datetime.utcnow()
        duration = int((end_time - start_time).total_seconds())

        async with db_manager.get_session() as session:
            scrape_log = await session.get(ScrapeLog, scrape_log_id)
            if scrape_log:
                scrape_log.completed_at = end_time
                scrape_log.duration_seconds = duration
                scrape_log.status = 'success'
                scrape_log.tokens_saved = result.get('total_saved', 0)
                scrape_log.tokens_skipped = result.get('total_skipped', 0)
                # ä» chains ç»“æœä¸­è®¡ç®— scraped æ€»æ•°
                total_scraped = sum(
                    chain_result.get('scraped', 0)
                    for chain_result in result.get('chains', {}).values()
                )
                scrape_log.tokens_scraped = total_scraped
                await session.commit()

        logger.info(f"âœ… å·²æ›´æ–°æŠ“å–æ—¥å¿—: è€—æ—¶ {duration}ç§’")

        logger.info(
            f"çˆ¬å–å®Œæˆï¼šæ€»å…±ä¿å­˜ {result['total_saved']} ä¸ªä»£å¸åˆ°æ•°æ®åº“ï¼Œ"
            f"è·³è¿‡ {result['total_skipped']} ä¸ª"
        )
        logger.info("="*80)

        # çˆ¬å–å®Œæˆåï¼Œç«‹å³æ›´æ–°æ½œåŠ›ä»£å¸çš„ AVE API æ•°æ®
        if result['total_saved'] > 0:
            logger.info("\n" + "="*80)
            logger.info("æ›´æ–°æ½œåŠ›ä»£å¸çš„ AVE API æ•°æ®...")
            logger.info("="*80)

            if not monitor_service:
                monitor_service = TokenMonitorService()

            update_result = await monitor_service.update_potential_tokens_data(
                delay=0.3,
                min_update_interval_minutes=0  # çˆ¬å–åç«‹å³æ›´æ–°ï¼Œä¸æ£€æŸ¥é—´éš”
            )

            logger.info(
                f"æ›´æ–°å®Œæˆï¼šæˆåŠŸ {update_result.get('updated', 0)} ä¸ªï¼Œ"
                f"å¤±è´¥ {update_result.get('failed', 0)} ä¸ª"
            )
            logger.info("="*80)

        # è°ƒåº¦ä¸‹ä¸€æ¬¡çˆ¬å–ä»»åŠ¡ï¼ˆä½¿ç”¨é…ç½®çš„é—´éš”æ—¶é—´ï¼‰
        schedule_next_scrape(config)

    except Exception as e:
        logger.error(f"çˆ¬å–ä»»åŠ¡å¤±è´¥: {e}", exc_info=True)

        # æ›´æ–° ScrapeLog è®°å½•ï¼ˆçŠ¶æ€ï¼šfailedï¼‰
        if scrape_log_id and db_manager:
            try:
                end_time = datetime.utcnow()
                duration = int((end_time - start_time).total_seconds())

                async with db_manager.get_session() as session:
                    scrape_log = await session.get(ScrapeLog, scrape_log_id)
                    if scrape_log:
                        scrape_log.completed_at = end_time
                        scrape_log.duration_seconds = duration
                        scrape_log.status = 'failed'
                        scrape_log.error_message = str(e)[:1000]  # é™åˆ¶é•¿åº¦
                        await session.commit()

                logger.info(f"âŒ å·²æ›´æ–°æŠ“å–æ—¥å¿—: å¤±è´¥ï¼Œè€—æ—¶ {duration}ç§’")
            except Exception as log_error:
                logger.error(f"æ›´æ–°å¤±è´¥æ—¥å¿—æ—¶å‡ºé”™: {log_error}")

        # å³ä½¿å¤±è´¥ä¹Ÿè¦è°ƒåº¦ä¸‹ä¸€æ¬¡ï¼ˆä½¿ç”¨é»˜è®¤é…ç½®ï¼‰
        schedule_next_scrape()

    finally:
        # å…³é—­è¿æ¥
        if scraper:
            await scraper.close()
        if monitor_service:
            await monitor_service.close()


def schedule_next_scrape(config=None):
    """
    è°ƒåº¦ä¸‹ä¸€æ¬¡çˆ¬å–ä»»åŠ¡ï¼ˆä½¿ç”¨é…ç½®çš„é—´éš”æ—¶é—´æˆ–é»˜è®¤9-15åˆ†é’Ÿï¼‰

    Args:
        config: çˆ¬è™«é…ç½®å­—å…¸ï¼ŒåŒ…å« scrape_interval_min å’Œ scrape_interval_max
    """
    global scheduler, enable_scraper

    # å¦‚æœçˆ¬è™«è¢«ç¦ç”¨ï¼Œä¸è°ƒåº¦
    if not enable_scraper:
        return

    if scheduler:
        # ä»é…ç½®è¯»å–é—´éš”æ—¶é—´ï¼Œå¦‚æœæ²¡æœ‰é…ç½®åˆ™ä½¿ç”¨é»˜è®¤å€¼ï¼ˆ9-15åˆ†é’Ÿï¼‰
        if config:
            interval_min = config.get('scrape_interval_min', 9)
            interval_max = config.get('scrape_interval_max', 15)
        else:
            interval_min = 9
            interval_max = 15

        # è®¡ç®—éšæœºé—´éš”æ—¶é—´
        next_run_minutes = random.uniform(interval_min, interval_max)
        next_run_time = datetime.now() + timedelta(minutes=next_run_minutes)

        # ç§»é™¤æ—§çš„çˆ¬å–ä»»åŠ¡
        try:
            scheduler.remove_job('scrape_dexscreener')
        except:
            pass

        # æ·»åŠ æ–°çš„ä¸€æ¬¡æ€§ä»»åŠ¡
        scheduler.add_job(
            scrape_dexscreener_task,
            trigger=DateTrigger(run_date=next_run_time),
            id='scrape_dexscreener',
            name='çˆ¬å–DexScreeneré¦–é¡µ',
            max_instances=1,
            coalesce=True,
            misfire_grace_time=60
        )

        logger.info(f"ğŸ“… ä¸‹æ¬¡çˆ¬å–æ—¶é—´: {next_run_time.strftime('%Y-%m-%d %H:%M:%S')} "
                   f"(é—´éš” {next_run_minutes:.1f} åˆ†é’Ÿ)")



async def monitor_prices_task():
    """
    ç›‘æ§ä»·æ ¼ä»»åŠ¡ï¼ˆä»æ•°æ®åº“è¯»å–é…ç½®ï¼‰
    æ›´æ–°ç›‘æ§ä»£å¸ä»·æ ¼ + æ½œåŠ›ä»£å¸æ•°æ®
    """
    from src.storage.models import MonitorLog
    from src.storage.db_manager import DatabaseManager
    import uuid

    global monitor_service

    monitor_log_id = None
    db_manager = None
    start_time = None

    try:
        # ä»»åŠ¡å¼€å§‹è®¡æ—¶ï¼ˆåŒ…å«æ‰€æœ‰æ­¥éª¤ï¼‰
        start_time = datetime.utcnow()

        logger.info("="*80)
        logger.info("å¼€å§‹æ›´æ–°ç›‘æ§ä»£å¸ä»·æ ¼...")
        logger.info("="*80)

        # 1. ä»æ•°æ®åº“è¯»å–ç›‘æ§é…ç½®
        if not monitor_service:
            monitor_service = TokenMonitorService()

        config = await monitor_service.get_monitor_config()

        if not config:
            logger.error("æœªæ‰¾åˆ°ç›‘æ§é…ç½®ï¼Œè·³è¿‡æœ¬æ¬¡æ›´æ–°")
            return

        # 2. æ£€æŸ¥é…ç½®æ˜¯å¦å¯ç”¨
        if not config.get('enabled', True):
            logger.info("ç›‘æ§é…ç½®å·²ç¦ç”¨ï¼Œè·³è¿‡æœ¬æ¬¡æ›´æ–°")
            return

        logger.info(f"é…ç½®ä¿¡æ¯: é—´éš”={config['update_interval_minutes']}åˆ†é’Ÿ, "
                   f"å¸‚å€¼é˜ˆå€¼={config.get('min_monitor_market_cap')}, "
                   f"æµåŠ¨æ€§é˜ˆå€¼={config.get('min_monitor_liquidity')}")

        # åˆ›å»º MonitorLog è®°å½•ï¼ˆçŠ¶æ€ï¼šrunningï¼‰
        db_manager = DatabaseManager()
        monitor_log_id = str(uuid.uuid4())

        async with db_manager.get_session() as session:
            monitor_log = MonitorLog(
                id=monitor_log_id,
                started_at=start_time,
                status='running',
                config_snapshot=config  # ä¿å­˜é…ç½®å¿«ç…§
            )
            session.add(monitor_log)
            await session.commit()

        logger.info(f"ğŸ“ å·²åˆ›å»ºç›‘æ§æ—¥å¿—è®°å½•: {monitor_log_id}")

        # æ›´æ–°æ‰€æœ‰ç›‘æ§ä»£å¸çš„ä»·æ ¼
        result = await monitor_service.update_monitored_prices()

        # æ›´æ–° MonitorLog è®°å½•ï¼ˆä¸­é—´çŠ¶æ€ï¼‰
        async with db_manager.get_session() as session:
            monitor_log = await session.get(MonitorLog, monitor_log_id)
            if monitor_log:
                monitor_log.status = 'success'
                monitor_log.tokens_monitored = result.get('total_monitored', 0)
                monitor_log.tokens_updated = result.get('updated', 0)
                monitor_log.tokens_failed = result.get('failed', 0)
                monitor_log.tokens_auto_removed = result.get('removed', 0)
                monitor_log.alerts_triggered = result.get('alerts_triggered', 0)
                monitor_log.removed_by_market_cap = result.get('removed_by_market_cap', 0)
                monitor_log.removed_by_liquidity = result.get('removed_by_liquidity', 0)
                await session.commit()

        logger.info(
            f"ä»·æ ¼æ›´æ–°å®Œæˆï¼šæ›´æ–° {result['updated']} ä¸ªä»£å¸ï¼Œ"
            f"è§¦å‘ {result['alerts_triggered']} ä¸ªæŠ¥è­¦"
        )
        if result.get('removed', 0) > 0:
            logger.info(
                f"è‡ªåŠ¨åˆ é™¤ {result['removed']} ä¸ªä»£å¸ "
                f"(å¸‚å€¼: {result.get('removed_by_market_cap', 0)}, "
                f"æµåŠ¨æ€§: {result.get('removed_by_liquidity', 0)})"
            )
        logger.info("="*80)

        # åŒæ—¶æ›´æ–°æ½œåŠ›ä»£å¸çš„ AVE API æ•°æ®ï¼ˆå¸¦å»é‡æ£€æŸ¥ï¼‰
        logger.info("\n" + "="*80)
        logger.info("æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°æ½œåŠ›ä»£å¸æ•°æ®...")
        logger.info("="*80)

        potential_result = await monitor_service.update_potential_tokens_data(
            delay=0.3,
            min_update_interval_minutes=3  # æœ€å°‘é—´éš”3åˆ†é’Ÿï¼Œé¿å…é‡å¤è°ƒç”¨
        )

        if potential_result.get('skipped'):
            logger.info("æ½œåŠ›ä»£å¸æ•°æ®æ›´æ–°å·²è·³è¿‡ï¼ˆè·ä¸Šæ¬¡æ›´æ–°æ—¶é—´å¤ªçŸ­ï¼‰")
        else:
            logger.info(
                f"æ½œåŠ›ä»£å¸æ›´æ–°å®Œæˆï¼šæˆåŠŸ {potential_result.get('updated', 0)} ä¸ªï¼Œ"
                f"å¤±è´¥ {potential_result.get('failed', 0)} ä¸ª"
            )

            # å¦‚æœæ½œåŠ›ä»£å¸æœ‰åˆ é™¤ï¼Œç´¯åŠ åˆ°ç›‘æ§æ—¥å¿—ç»Ÿè®¡ä¸­
            potential_removed = potential_result.get('removed', 0)
            if potential_removed > 0:
                logger.info(
                    f"è‡ªåŠ¨åˆ é™¤æ½œåŠ›ä»£å¸ {potential_removed} ä¸ª "
                    f"(å¸‚å€¼: {potential_result.get('removed_by_market_cap', 0)}, "
                    f"æµåŠ¨æ€§: {potential_result.get('removed_by_liquidity', 0)})"
                )

                # æ›´æ–° monitor_logï¼Œç´¯åŠ æ½œåŠ›ä»£å¸çš„åˆ é™¤ç»Ÿè®¡
                async with db_manager.get_session() as session:
                    monitor_log = await session.get(MonitorLog, monitor_log_id)
                    if monitor_log:
                        monitor_log.tokens_auto_removed += potential_removed
                        monitor_log.removed_by_market_cap += potential_result.get('removed_by_market_cap', 0)
                        monitor_log.removed_by_liquidity += potential_result.get('removed_by_liquidity', 0)
                        await session.commit()

                logger.info(f"âœ… å·²ç´¯åŠ æ½œåŠ›ä»£å¸åˆ é™¤ç»Ÿè®¡åˆ°ç›‘æ§æ—¥å¿—")

        # æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼Œè®¡ç®—æ€»è€—æ—¶å¹¶æ›´æ–°ç›‘æ§æ—¥å¿—
        end_time = datetime.utcnow()
        duration = int((end_time - start_time).total_seconds())

        async with db_manager.get_session() as session:
            monitor_log = await session.get(MonitorLog, monitor_log_id)
            if monitor_log:
                monitor_log.completed_at = end_time
                monitor_log.duration_seconds = duration
                await session.commit()

        logger.info(f"âœ… ç›‘æ§ä»»åŠ¡å®Œæˆï¼Œæ€»è€—æ—¶: {duration} ç§’")
        logger.info("="*80)

    except Exception as e:
        logger.error(f"ç›‘æ§ä»»åŠ¡å¤±è´¥: {e}", exc_info=True)

        # æ›´æ–° MonitorLog è®°å½•ï¼ˆçŠ¶æ€ï¼šfailedï¼‰
        if monitor_log_id and db_manager and start_time:
            try:
                end_time = datetime.utcnow()
                duration = int((end_time - start_time).total_seconds())

                async with db_manager.get_session() as session:
                    monitor_log = await session.get(MonitorLog, monitor_log_id)
                    if monitor_log:
                        monitor_log.completed_at = end_time
                        monitor_log.duration_seconds = duration
                        monitor_log.status = 'failed'
                        monitor_log.error_message = str(e)[:1000]  # é™åˆ¶é•¿åº¦
                        await session.commit()

                logger.info(f"âŒ å·²æ›´æ–°ç›‘æ§æ—¥å¿—: å¤±è´¥ï¼Œè€—æ—¶ {duration}ç§’")
            except Exception as log_error:
                logger.error(f"æ›´æ–°å¤±è´¥æ—¥å¿—æ—¶å‡ºé”™: {log_error}")
    finally:
        # å…³é—­æ•°æ®åº“è¿æ¥
        if db_manager:
            try:
                await db_manager.close()
            except:
                pass


async def update_klines_task():
    """
    æ›´æ–°Kçº¿æ•°æ®ä»»åŠ¡ï¼ˆæ¯1å°æ—¶ï¼‰
    æ‹‰å–æ‰€æœ‰ç›‘æ§ä»£å¸å’Œæ½œåŠ›ä»£å¸çš„Kçº¿æ•°æ®
    """
    kline_service = None

    try:
        logger.info("="*80)
        logger.info("å¼€å§‹æ›´æ–°Kçº¿æ•°æ®...")
        logger.info("="*80)

        kline_service = KlineService()

        # è°ƒç”¨ç»Ÿä¸€æ›´æ–°æ–¹æ³•ï¼ˆå†…éƒ¨è‡ªåŠ¨é™æµï¼‰
        result = await kline_service.update_all_tokens_klines(
            timeframe="minute",
            aggregate=5,
            max_candles=500
        )

        logger.info("="*80)
        logger.info("âœ… Kçº¿æ•°æ®æ›´æ–°å®Œæˆ")
        logger.info(f"  ç›‘æ§ä»£å¸: {result['monitored']} ä¸ª")
        logger.info(f"  æ½œåŠ›ä»£å¸: {result['potential']} ä¸ª")
        logger.info(f"  æ€»ä»£å¸æ•°: {result['total']} ä¸ª")
        logger.info(f"  æˆåŠŸ: {result['success']} ä¸ªï¼Œå¤±è´¥: {result['failed']} ä¸ª")
        logger.info(f"  æ‹‰å–: {result['total_fetched']} æ ¹ï¼Œä¿å­˜: {result['total_saved']} æ ¹")
        logger.info("="*80)

    except Exception as e:
        logger.error(f"âŒ æ›´æ–°Kçº¿æ•°æ®æ—¶å‡ºé”™: {e}", exc_info=True)


def shutdown_handler(signum, frame):
    """
    ä¼˜é›…å…³é—­å¤„ç†
    """
    global scheduler
    logger.info("æ”¶åˆ°å…³é—­ä¿¡å·ï¼Œæ­£åœ¨å…³é—­è°ƒåº¦å™¨...")

    if scheduler:
        try:
            scheduler.shutdown(wait=False)
            logger.info("è°ƒåº¦å™¨å·²å…³é—­")
        except Exception:
            pass  # å¿½ç•¥é‡å¤å…³é—­çš„é”™è¯¯

    # ç›´æ¥é€€å‡ºï¼ˆé¿å…ç¨‹åºå¡æ­»ï¼‰
    import os
    os._exit(0)


async def main():
    """
    ä¸»å‡½æ•°ï¼šå¯åŠ¨è°ƒåº¦å™¨
    """
    global scheduler, monitor_service, enable_scraper

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(
        description='å®šæ—¶ä»»åŠ¡å®ˆæŠ¤è¿›ç¨‹ï¼šç›‘æ§ä»£å¸ä»·æ ¼å’Œçˆ¬å– DexScreener æ•°æ®',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•ï¼š
  python scheduler_daemon.py                   # å¯åŠ¨æ‰€æœ‰ä»»åŠ¡ï¼ˆçˆ¬è™«+ç›‘æ§ï¼‰
  python scheduler_daemon.py --monitor-only    # åªå¯åŠ¨ç›‘æ§ä»»åŠ¡ï¼ˆä¸çˆ¬å–ï¼‰
  python scheduler_daemon.py --scraper-only    # åªå¯åŠ¨çˆ¬è™«ä»»åŠ¡ï¼ˆä¸ç›‘æ§ï¼‰
        """
    )
    parser.add_argument(
        '--monitor-only',
        action='store_true',
        help='åªå¯åŠ¨ç›‘æ§ä»·æ ¼ä»»åŠ¡ï¼Œä¸å¯åŠ¨çˆ¬è™«'
    )
    parser.add_argument(
        '--scraper-only',
        action='store_true',
        help='åªå¯åŠ¨çˆ¬è™«ä»»åŠ¡ï¼Œä¸å¯åŠ¨ç›‘æ§'
    )
    parser.add_argument(
        '--use-undetected-chrome',
        action='store_true',
        help='ä½¿ç”¨ undetected-chromedriver çˆ¬å–ï¼ˆæˆåŠŸç‡æ›´é«˜ï¼Œéœ€è¦å®‰è£… Chromeï¼‰'
    )

    args = parser.parse_args()

    # æ£€æŸ¥å‚æ•°å†²çª
    if args.monitor_only and args.scraper_only:
        logger.error("é”™è¯¯: --monitor-only å’Œ --scraper-only ä¸èƒ½åŒæ—¶ä½¿ç”¨")
        sys.exit(1)

    # ç¡®å®šå¯ç”¨å“ªäº›ä»»åŠ¡
    enable_scraper = not args.monitor_only
    enable_monitor = not args.scraper_only

    # è®¾ç½®çˆ¬å–æ–¹æ³•
    global use_undetected_chrome
    if args.use_undetected_chrome:
        use_undetected_chrome = True

    logger.info("="*80)
    logger.info("å®šæ—¶ä»»åŠ¡å®ˆæŠ¤è¿›ç¨‹å¯åŠ¨")
    if use_undetected_chrome:
        logger.info("çˆ¬å–æ–¹æ³•: undetected-chromedriverï¼ˆé«˜æˆåŠŸç‡æ¨¡å¼ï¼‰")
    else:
        logger.info("çˆ¬å–æ–¹æ³•: cloudscraperï¼ˆå¿«é€Ÿæ¨¡å¼ï¼‰")
    logger.info("="*80)

    # æ³¨å†Œä¿¡å·å¤„ç†
    signal.signal(signal.SIGINT, shutdown_handler)
    signal.signal(signal.SIGTERM, shutdown_handler)

    # åˆå§‹åŒ–æœåŠ¡
    monitor_service = TokenMonitorService()

    # åˆ›å»ºè°ƒåº¦å™¨
    scheduler = AsyncIOScheduler(timezone='UTC')

    # æ ¹æ®å‚æ•°æ·»åŠ ä»»åŠ¡
    if enable_monitor:
        # ä»æ•°æ®åº“è¯»å–ç›‘æ§é…ç½®
        monitor_config = await monitor_service.get_monitor_config()

        if not monitor_config:
            logger.error("æœªæ‰¾åˆ°ç›‘æ§é…ç½®ï¼Œä½¿ç”¨é»˜è®¤é—´éš” 5 åˆ†é’Ÿ")
            update_interval = 5
        else:
            update_interval = monitor_config.get('update_interval_minutes', 5)
            logger.info(f"ä»é…ç½®è¯»å–æ›´æ–°é—´éš”: {update_interval} åˆ†é’Ÿ")

        scheduler.add_job(
            monitor_prices_task,
            trigger=IntervalTrigger(minutes=update_interval),
            id='monitor_prices',
            name='ç›‘æ§ä»£å¸ä»·æ ¼',
            max_instances=1,
            coalesce=True,
            misfire_grace_time=30
        )
        logger.info(f"âœ… å·²å¯ç”¨ä»»åŠ¡ï¼šæ¯ {update_interval} åˆ†é’Ÿç›‘æ§ä»£å¸ä»·æ ¼")

        # Kçº¿æ›´æ–°ä»»åŠ¡ï¼šæ¯1å°æ—¶æ‰§è¡Œä¸€æ¬¡
        scheduler.add_job(
            update_klines_task,
            trigger=IntervalTrigger(hours=1),
            id='update_klines',
            name='æ›´æ–°Kçº¿æ•°æ®',
            max_instances=1,
            coalesce=True,
            misfire_grace_time=300  # å…è®¸5åˆ†é’Ÿçš„å»¶è¿Ÿ
        )
        logger.info("âœ… å·²å¯ç”¨ä»»åŠ¡ï¼šæ¯1å°æ—¶æ›´æ–°Kçº¿æ•°æ®")

    # å¯åŠ¨è°ƒåº¦å™¨
    scheduler.start()

    logger.info("è°ƒåº¦å™¨å·²å¯åŠ¨ï¼Œä»»åŠ¡è®¡åˆ’ï¼š")
    if enable_scraper:
        logger.info("  - éšæœºé—´éš”9-15åˆ†é’Ÿçˆ¬å– DexScreener é¦–é¡µï¼ˆBSC + Solanaï¼Œæ”¯æŒé‡è¯•æœºåˆ¶ï¼‰")
    if enable_monitor:
        logger.info(f"  - æ¯ {update_interval} åˆ†é’Ÿç›‘æ§ä»£å¸ä»·æ ¼ï¼ˆæ›´æ–° monitored_tokens è¡¨å¹¶è§¦å‘æŠ¥è­¦ + æ›´æ–° potential_tokens AVE æ•°æ®ï¼‰")
        logger.info("  - æ¯1å°æ—¶æ›´æ–°Kçº¿æ•°æ®ï¼ˆç›‘æ§ä»£å¸ + æ½œåŠ›ä»£å¸ï¼Œ5åˆ†é’ŸKçº¿ï¼‰")
    logger.info("="*80)

    # å¯åŠ¨æ—¶ç«‹å³æ‰§è¡Œä¸€æ¬¡ä»»åŠ¡
    if enable_scraper:
        logger.info("ç«‹å³æ‰§è¡Œä¸€æ¬¡çˆ¬å–ä»»åŠ¡...")
        await scrape_dexscreener_task()

    if enable_monitor:
        logger.info("ç«‹å³æ‰§è¡Œä¸€æ¬¡ç›‘æ§ä»»åŠ¡...")
        await monitor_prices_task()

        # å¯åŠ¨æ—¶ä¹Ÿç«‹å³æ‰§è¡Œä¸€æ¬¡Kçº¿æ›´æ–°
        logger.info("ç«‹å³æ‰§è¡Œä¸€æ¬¡Kçº¿æ›´æ–°ä»»åŠ¡...")
        await update_klines_task()

    # ä¿æŒè¿è¡Œ
    try:
        while True:
            await asyncio.sleep(60)
    except (KeyboardInterrupt, SystemExit):
        logger.info("æ¥æ”¶åˆ°é€€å‡ºä¿¡å·")
    except Exception as e:
        logger.error(f"è¿è¡Œæ—¶é”™è¯¯: {e}", exc_info=True)
    finally:
        logger.info("æ­£åœ¨å…³é—­æœåŠ¡...")
        if scheduler:
            try:
                scheduler.shutdown(wait=False)
            except:
                pass
        if monitor_service:
            try:
                await monitor_service.close()
            except:
                pass
        logger.info("âœ… å®šæ—¶ä»»åŠ¡å®ˆæŠ¤è¿›ç¨‹å·²å®‰å…¨å…³é—­")


if __name__ == "__main__":
    asyncio.run(main())
