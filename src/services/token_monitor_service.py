#!/usr/bin/env python3
"""
Token Monitor Service
ç›‘æ§ä»£å¸ä»·æ ¼å˜åŒ–ï¼Œè§¦å‘æŠ¥è­¦
"""

import asyncio
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
from decimal import Decimal

from sqlalchemy import select, and_, desc
from sqlalchemy.ext.asyncio import AsyncSession

from src.storage.db_manager import DatabaseManager
from src.storage.models import MonitoredToken, PriceAlert, DexScreenerToken, PotentialToken, ScraperConfig, MonitorConfig
from src.services.dexscreener_service import DexScreenerService
from src.services.ave_api_service import ave_api_service
from src.utils.logger import setup_logger

logger = setup_logger(__name__)


class TokenMonitorService:
    """Token monitoring service for price drop alerts."""

    def __init__(self, db_manager: Optional[DatabaseManager] = None):
        """
        Initialize monitor service.

        Args:
            db_manager: Database manager instance
        """
        self.db_manager = db_manager
        self._db_created = False
        self.dex_service = DexScreenerService(db_manager=db_manager)

    async def _ensure_db(self):
        """Ensure database manager is initialized."""
        if self.db_manager is None:
            self.db_manager = DatabaseManager()
            await self.db_manager.init_async_db()
            self._db_created = True

    async def close(self):
        """Close database connection."""
        if self._db_created and self.db_manager:
            await self.db_manager.close()

    def _format_token_list(self, tokens: list) -> List[Dict[str, Any]]:
        """
        Format a list of MonitoredToken objects to dictionaries with all AVE API fields.

        Args:
            tokens: List of MonitoredToken objects

        Returns:
            List of token dictionaries
        """
        return [
            {
                # åŸºç¡€ä¿¡æ¯
                "id": token.id,
                "token_address": token.token_address,
                "token_symbol": token.token_symbol,
                "token_name": token.token_name,
                "chain": getattr(token, 'chain', 'bsc'),  # å…¼å®¹æ—§æ•°æ®
                "dex_id": token.dex_id,
                "pair_address": token.pair_address,
                "amm": token.amm,
                "dex_type": getattr(token, 'dex_type', None),  # å…¼å®¹æ—§æ•°æ®

                # ä»·æ ¼ä¿¡æ¯
                "entry_price_usd": float(token.entry_price_usd),
                "current_price_usd": float(token.current_price_usd) if token.current_price_usd else None,
                "peak_price_usd": float(token.peak_price_usd),
                "price_ath_usd": float(token.price_ath_usd) if token.price_ath_usd else None,

                # è®¡ç®—å­—æ®µï¼ˆä»å†å²ATHåˆ°å½“å‰ï¼‰
                "drop_from_peak_percent": (
                    float((token.price_ath_usd - token.current_price_usd) / token.price_ath_usd * 100)
                    if token.current_price_usd and token.price_ath_usd and token.price_ath_usd > 0
                    else (
                        float((token.peak_price_usd - token.current_price_usd) / token.peak_price_usd * 100)
                        if token.current_price_usd and token.peak_price_usd > 0 else None
                    )
                ),
                "multiplier_to_peak": (
                    float(token.price_ath_usd / token.current_price_usd)
                    if token.current_price_usd and token.price_ath_usd and token.current_price_usd > 0
                    else (
                        float(token.peak_price_usd / token.current_price_usd)
                        if token.current_price_usd and token.current_price_usd > 0 else None
                    )
                ),

                # æ—¶é—´æˆ³
                "entry_timestamp": token.entry_timestamp.isoformat() if token.entry_timestamp else None,
                "last_update_timestamp": token.last_update_timestamp.isoformat() if token.last_update_timestamp else None,
                "peak_timestamp": token.peak_timestamp.isoformat() if token.peak_timestamp else None,
                "token_created_at": token.token_created_at.isoformat() if token.token_created_at else None,
                "first_trade_at": token.first_trade_at.isoformat() if token.first_trade_at else None,

                # å¸‚åœºæ•°æ®
                "current_tvl": float(token.current_tvl) if token.current_tvl else None,
                "current_market_cap": float(token.current_market_cap) if token.current_market_cap else None,
                "market_cap_at_entry": float(token.market_cap_at_entry) if token.market_cap_at_entry else None,
                "liquidity_at_entry": float(token.liquidity_at_entry) if token.liquidity_at_entry else None,
                "volume_24h_at_entry": float(token.volume_24h_at_entry) if token.volume_24h_at_entry else None,
                "price_change_24h_at_entry": float(token.price_change_24h_at_entry) if token.price_change_24h_at_entry else None,

                # ä»·æ ¼å˜åŒ–ï¼ˆå¤šæ—¶é—´æ®µï¼‰
                "price_change_1m": float(token.price_change_1m) if token.price_change_1m else None,
                "price_change_5m": float(token.price_change_5m) if token.price_change_5m else None,
                "price_change_15m": float(token.price_change_15m) if token.price_change_15m else None,
                "price_change_30m": float(token.price_change_30m) if token.price_change_30m else None,
                "price_change_1h": float(token.price_change_1h) if token.price_change_1h else None,
                "price_change_4h": float(token.price_change_4h) if token.price_change_4h else None,
                "price_change_24h": float(token.price_change_24h) if token.price_change_24h else None,

                # äº¤æ˜“é‡ï¼ˆå¤šæ—¶é—´æ®µï¼‰
                "volume_1m": float(token.volume_1m) if token.volume_1m else None,
                "volume_5m": float(token.volume_5m) if token.volume_5m else None,
                "volume_15m": float(token.volume_15m) if token.volume_15m else None,
                "volume_30m": float(token.volume_30m) if token.volume_30m else None,
                "volume_1h": float(token.volume_1h) if token.volume_1h else None,
                "volume_4h": float(token.volume_4h) if token.volume_4h else None,
                "volume_24h": float(token.volume_24h) if token.volume_24h else None,

                # äº¤æ˜“æ¬¡æ•°ï¼ˆå¤šæ—¶é—´æ®µï¼‰
                "tx_count_1m": token.tx_count_1m,
                "tx_count_5m": token.tx_count_5m,
                "tx_count_15m": token.tx_count_15m,
                "tx_count_30m": token.tx_count_30m,
                "tx_count_1h": token.tx_count_1h,
                "tx_count_4h": token.tx_count_4h,
                "tx_count_24h": token.tx_count_24h,

                # ä¹°å–æ•°æ®
                "buys_24h": token.buys_24h,
                "sells_24h": token.sells_24h,

                # äº¤æ˜“è€…æ•°æ®
                "makers_24h": token.makers_24h,
                "buyers_24h": token.buyers_24h,
                "sellers_24h": token.sellers_24h,

                # 24å°æ—¶ä»·æ ¼èŒƒå›´
                "price_24h_high": float(token.price_24h_high) if token.price_24h_high else None,
                "price_24h_low": float(token.price_24h_low) if token.price_24h_low else None,
                "open_price_24h": float(token.open_price_24h) if token.open_price_24h else None,

                # LPä¿¡æ¯
                "lp_holders": token.lp_holders,
                "lp_locked_percent": float(token.lp_locked_percent) if token.lp_locked_percent else None,
                "lp_lock_platform": token.lp_lock_platform,

                # å®‰å…¨æŒ‡æ ‡
                "rusher_tx_count": token.rusher_tx_count,
                "sniper_tx_count": token.sniper_tx_count,

                # Tokenåˆ›å»ºä¿¡æ¯
                "creation_block_number": token.creation_block_number,
                "creation_tx_hash": token.creation_tx_hash,

                # ç›‘æ§çŠ¶æ€
                "status": token.status,
                "drop_threshold_percent": float(token.drop_threshold_percent),
                "alert_thresholds": token.alert_thresholds if token.alert_thresholds else [70, 80, 90],
            }
            for token in tokens
        ]

    def _apply_token_filters(
        self,
        tokens: List[Dict[str, Any]],
        filter_config: Dict[str, Any]
    ) -> tuple[List[Dict[str, Any]], Dict[str, int]]:
        """
        åº”ç”¨ä»£å¸è¿‡æ»¤æ¡ä»¶

        Args:
            tokens: ä»£å¸åˆ—è¡¨ï¼ˆDexScreeneræ ¼å¼ï¼‰
            filter_config: è¿‡æ»¤é…ç½®

        Returns:
            (è¿‡æ»¤åçš„ä»£å¸åˆ—è¡¨, è¿‡æ»¤ç»Ÿè®¡ä¿¡æ¯)
        """
        min_market_cap = filter_config.get('min_market_cap')
        min_liquidity = filter_config.get('min_liquidity')
        max_token_age_days = filter_config.get('max_token_age_days')

        filtered_tokens = []
        stats = {
            'by_market_cap': 0,
            'by_liquidity': 0,
            'by_age': 0
        }

        current_time = datetime.utcnow()

        for token in tokens:
            # æ£€æŸ¥å¸‚å€¼
            if min_market_cap is not None:
                market_cap = token.get('fdv') or token.get('marketCap')
                if market_cap is None or float(market_cap) < float(min_market_cap):
                    stats['by_market_cap'] += 1
                    continue

            # æ£€æŸ¥æµåŠ¨æ€§
            if min_liquidity is not None:
                liquidity = token.get('liquidity', {}).get('usd')
                if liquidity is None or float(liquidity) < float(min_liquidity):
                    stats['by_liquidity'] += 1
                    continue

            # æ£€æŸ¥ä»£å¸å¹´é¾„
            if max_token_age_days is not None:
                pair_created_at = token.get('pairCreatedAt')
                if pair_created_at:
                    try:
                        # pairCreatedAt å¯èƒ½æ˜¯æ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰
                        created_timestamp = int(pair_created_at) / 1000 if pair_created_at > 1000000000000 else int(pair_created_at)
                        created_time = datetime.fromtimestamp(created_timestamp)
                        age_days = (current_time - created_time).total_seconds() / 86400

                        if age_days > max_token_age_days:
                            stats['by_age'] += 1
                            continue
                    except Exception as e:
                        logger.warning(f"Failed to parse pairCreatedAt: {pair_created_at}, error: {e}")

            # é€šè¿‡æ‰€æœ‰è¿‡æ»¤æ¡ä»¶
            filtered_tokens.append(token)

        return filtered_tokens, stats

    def scrape_and_filter_top_gainers(
        self,
        count: int = 100,
        top_n: int = 10,
        headless: bool = False,
        filter_config: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        ã€ç‹¬ç«‹åŠŸèƒ½1ã€‘åªè´Ÿè´£çˆ¬å–å’Œç­›é€‰Topæ¶¨å¹…ä»£å¸ï¼Œä¸æ·»åŠ åˆ°ç›‘æ§è¡¨

        Args:
            count: çˆ¬å–ä»£å¸æ•°é‡
            top_n: ç­›é€‰å‰Nå
            headless: æ˜¯å¦ä½¿ç”¨æ— å¤´æµè§ˆå™¨ï¼ˆå»ºè®®Falseä»¥ç»•è¿‡Cloudflareï¼‰
            filter_config: è¿‡æ»¤é…ç½® {min_market_cap, min_liquidity, max_token_age_days}

        Returns:
            å­—å…¸åŒ…å«ï¼štop_gainersï¼ˆå‰Nåä»£å¸åˆ—è¡¨ï¼‰å’Œè¿‡æ»¤ç»Ÿè®¡ä¿¡æ¯
        """
        logger.info(f"\n{'='*60}")
        logger.info(f"ã€çˆ¬å–ç­›é€‰ã€‘å¼€å§‹çˆ¬å–å¹¶ç­›é€‰ Top {top_n} æ¶¨å¹…ä»£å¸")
        logger.info(f"{'='*60}\n")

        # ä¸€æ¬¡æ€§ä»é¡µé¢è§£æå®Œæ•´æ•°æ®
        detailed_tokens = self.dex_service.scrape_bsc_page_with_details(
            target_count=count,
            headless=headless
        )

        if not detailed_tokens:
            logger.warning("æœªè·å–åˆ°ä»£å¸æ•°æ®")
            return {
                "top_gainers": [],
                "scraped_count": 0,
                "filtered_count": 0,
                "filter_stats": {}
            }

        logger.info(f"âœ“ å·²çˆ¬å– {len(detailed_tokens)} ä¸ªä»£å¸ï¼ˆå«å®Œæ•´æ•°æ®ï¼‰")

        # è¿‡æ»¤å‡ºæœ‰24hæ¶¨å¹…æ•°æ®çš„ä»£å¸
        tokens_with_change = [
            t for t in detailed_tokens
            if t.get('priceChange', {}).get('h24') is not None
        ]

        logger.info(f"âœ“ å…¶ä¸­ {len(tokens_with_change)} ä¸ªæœ‰24hæ¶¨å¹…æ•°æ®")

        # ã€æ–°å¢ã€‘åº”ç”¨è¿‡æ»¤æ¡ä»¶
        filtered_tokens, filter_stats = self._apply_token_filters(
            tokens_with_change,
            filter_config or {}
        )

        logger.info(f"âœ“ è¿‡æ»¤åå‰©ä½™ {len(filtered_tokens)} ä¸ªä»£å¸")
        if filter_stats:
            logger.info(f"   - å› å¸‚å€¼è¿‡æ»¤: {filter_stats.get('by_market_cap', 0)} ä¸ª")
            logger.info(f"   - å› æµåŠ¨æ€§è¿‡æ»¤: {filter_stats.get('by_liquidity', 0)} ä¸ª")
            logger.info(f"   - å› å¹´é¾„è¿‡æ»¤: {filter_stats.get('by_age', 0)} ä¸ª")

        # æŒ‰24hæ¶¨å¹…æ’åº
        sorted_tokens = sorted(
            filtered_tokens,
            key=lambda x: float(x.get('priceChange', {}).get('h24', 0)),
            reverse=True
        )

        top_gainers = sorted_tokens[:top_n]

        logger.info(f"\n{'='*60}")
        logger.info(f"Top {len(top_gainers)} æ¶¨å¹…æ¦œ:")
        logger.info(f"{'='*60}")
        for idx, token in enumerate(top_gainers, 1):
            symbol = token.get('baseToken', {}).get('symbol', 'UNKNOWN')
            change = token.get('priceChange', {}).get('h24', 0)
            price = token.get('priceUsd', '0')
            logger.info(f"{idx:2d}. {symbol:12s} +{change:>7.2f}%  ä»·æ ¼: ${price}")
        logger.info(f"{'='*60}\n")

        return {
            "top_gainers": top_gainers,
            "scraped_count": len(detailed_tokens),
            "filtered_count": len(filtered_tokens),
            "filter_stats": filter_stats
        }

    async def add_tokens_to_monitor(
        self,
        tokens: List[Dict[str, Any]],
        drop_threshold: float = 20.0
    ) -> Dict[str, int]:
        """
        ã€ç‹¬ç«‹åŠŸèƒ½2ã€‘å°†ä»£å¸åˆ—è¡¨æ·»åŠ åˆ°ç›‘æ§è¡¨

        Args:
            tokens: ä»£å¸æ•°æ®åˆ—è¡¨ï¼ˆå¯ä»¥æ¥è‡ª scrape_and_filter_top_gainers çš„è¿”å›å€¼ï¼‰
            drop_threshold: è·Œå¹…é˜ˆå€¼ï¼ˆç™¾åˆ†æ¯”ï¼‰

        Returns:
            ç»Ÿè®¡ä¿¡æ¯ {"total": int, "added": int, "skipped": int}
        """
        await self._ensure_db()

        logger.info(f"\n{'='*60}")
        logger.info(f"ã€æ·»åŠ ç›‘æ§ã€‘å¼€å§‹æ·»åŠ  {len(tokens)} ä¸ªä»£å¸åˆ°ç›‘æ§è¡¨")
        logger.info(f"{'='*60}\n")

        added_count = 0
        skipped_count = 0

        async with self.db_manager.get_session() as session:
            for token in tokens:
                try:
                    # æå–ä»£å¸æ•°æ®
                    base_token = token.get('baseToken', {})
                    token_address = base_token.get('address', '').lower()
                    token_symbol = base_token.get('symbol', 'UNKNOWN')
                    token_name = base_token.get('name', 'Unknown')

                    pair_address = token.get('pairAddress', '')
                    dex_id = token.get('dexId', 'unknown')

                    price_usd = float(token.get('priceUsd', 0))
                    price_change_24h = float(token.get('priceChange', {}).get('h24', 0))
                    market_cap = float(token.get('fdv', 0)) if token.get('fdv') else None
                    liquidity = float(token.get('liquidity', {}).get('usd', 0)) if token.get('liquidity') else None
                    volume_24h = float(token.get('volume', {}).get('h24', 0)) if token.get('volume') else None

                    if not token_address or not pair_address or price_usd <= 0:
                        logger.warning(f"æ•°æ®æ— æ•ˆ: {token_symbol}, è·³è¿‡")
                        skipped_count += 1
                        continue

                    # æ£€æŸ¥æ˜¯å¦å·²åœ¨ç›‘æ§ä¸­
                    existing = await session.execute(
                        select(MonitoredToken).where(
                            and_(
                                MonitoredToken.token_address == token_address,
                                MonitoredToken.status == "active"
                            )
                        )
                    )
                    if existing.scalar_one_or_none():
                        logger.info(f"  {token_symbol} å·²åœ¨ç›‘æ§ä¸­ï¼Œè·³è¿‡")
                        skipped_count += 1
                        continue

                    # åˆ›å»ºç›‘æ§è®°å½•
                    monitored = MonitoredToken(
                        token_address=token_address,
                        token_symbol=token_symbol,
                        token_name=token_name,
                        dex_id=dex_id,
                        pair_address=pair_address,
                        entry_price_usd=Decimal(str(price_usd)),
                        entry_timestamp=datetime.utcnow(),
                        current_price_usd=Decimal(str(price_usd)),
                        peak_price_usd=Decimal(str(price_usd)),
                        peak_timestamp=datetime.utcnow(),
                        market_cap_at_entry=Decimal(str(market_cap)) if market_cap else None,
                        liquidity_at_entry=Decimal(str(liquidity)) if liquidity else None,
                        volume_24h_at_entry=Decimal(str(volume_24h)) if volume_24h else None,
                        price_change_24h_at_entry=Decimal(str(price_change_24h)),
                        status="active",
                        drop_threshold_percent=Decimal(str(drop_threshold))
                    )

                    session.add(monitored)
                    added_count += 1

                    logger.info(
                        f"  âœ“ {token_symbol:12s} å…¥åœºä»·=${price_usd:.8f}, æ¶¨å¹…=+{price_change_24h:.2f}%"
                    )

                except Exception as e:
                    logger.error(f"æ·»åŠ ä»£å¸æ—¶å‡ºé”™: {e}")
                    skipped_count += 1
                    continue

            await session.commit()

        logger.info(f"\n{'='*60}")
        logger.info(f"æ·»åŠ å®Œæˆ: {added_count}/{len(tokens)} æˆåŠŸ, {skipped_count} è·³è¿‡")
        logger.info(f"{'='*60}\n")

        return {
            "total": len(tokens),
            "added": added_count,
            "skipped": skipped_count
        }

    async def scrape_and_add_top_gainers(
        self,
        count: int = 100,
        top_n: int = 10,
        drop_threshold: float = 20.0,
        headless: bool = False
    ) -> Dict[str, Any]:
        """
        ã€ä¸€é”®æ“ä½œã€‘çˆ¬å– + ç­›é€‰ + æ·»åŠ ç›‘æ§ï¼ˆä¾¿æ·æ–¹æ³•ï¼‰

        å¦‚æœæƒ³åˆ†å¼€æ‰§è¡Œï¼Œè¯·ä½¿ç”¨ï¼š
        - scrape_and_filter_top_gainers() - åªçˆ¬å–ç­›é€‰
        - add_tokens_to_monitor() - åªæ·»åŠ ç›‘æ§

        Args:
            count: çˆ¬å–ä»£å¸æ•°é‡
            top_n: ç­›é€‰å‰Nå
            drop_threshold: è·Œå¹…é˜ˆå€¼
            headless: æ˜¯å¦ä½¿ç”¨æ— å¤´æµè§ˆå™¨

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        logger.info("\n" + "="*60)
        logger.info("ã€ä¸€é”®æ“ä½œã€‘çˆ¬å– + ç­›é€‰ + æ·»åŠ ç›‘æ§")
        logger.info("="*60)

        # æ­¥éª¤1: çˆ¬å–å’Œç­›é€‰
        scrape_result = self.scrape_and_filter_top_gainers(
            count=count,
            top_n=top_n,
            headless=headless
        )

        top_gainers = scrape_result["top_gainers"]
        if not top_gainers:
            return {
                "scraped": scrape_result["scraped_count"],
                "top_gainers": 0,
                "saved": 0,
                "skipped": 0,
                "filter_stats": scrape_result.get("filter_stats", {})
            }

        # æ­¥éª¤2: æ·»åŠ åˆ°ç›‘æ§
        add_result = await self.add_tokens_to_monitor(
            tokens=top_gainers,
            drop_threshold=drop_threshold
        )

        return {
            "scraped": scrape_result["scraped_count"],
            "filtered": scrape_result["filtered_count"],
            "top_gainers": len(top_gainers),
            "added": add_result["added"],
            "skipped": add_result["skipped"],
            "filter_stats": scrape_result.get("filter_stats", {})
        }

    async def update_monitored_prices(
        self,
        batch_size: int = 10,
        delay: float = 0.3
    ) -> Dict[str, Any]:
        """
        Update current prices and detailed data for all active monitored tokens using AVE API.

        Args:
            batch_size: Number of pairs to fetch at once (not used with AVE API, kept for compatibility)
            delay: Delay between API calls (seconds)

        Returns:
            Update statistics including removal counts
        """
        await self._ensure_db()

        logger.info("Updating prices for monitored tokens using AVE API...")

        # Load monitor configuration
        monitor_config = None
        async with self.db_manager.get_session() as session:
            config_result = await session.execute(
                select(MonitorConfig).limit(1)
            )
            monitor_config = config_result.scalar_one_or_none()

        min_market_cap = None
        min_liquidity = None
        if monitor_config:
            min_market_cap = float(monitor_config.min_monitor_market_cap) if monitor_config.min_monitor_market_cap else None
            min_liquidity = float(monitor_config.min_monitor_liquidity) if monitor_config.min_monitor_liquidity else None
            if min_market_cap or min_liquidity:
                logger.info(f"ç›‘æ§è¿‡æ»¤é˜ˆå€¼: å¸‚å€¼ >= {min_market_cap}, æµåŠ¨æ€§ >= {min_liquidity}")

        # Get all monitored tokens (active and alerted, but not stopped or deleted)
        # å·²è§¦å‘æŠ¥è­¦çš„ä»£å¸ä¹Ÿè¦ç»§ç»­æ›´æ–°ï¼Œå› ä¸ºå¯èƒ½æœ‰å¤šçº§é˜ˆå€¼
        async with self.db_manager.get_session() as session:
            result = await session.execute(
                select(MonitoredToken).where(
                    MonitoredToken.status != "stopped",
                    MonitoredToken.deleted_at.is_(None),
                    MonitoredToken.permanently_deleted == 0
                )
            )
            monitored_tokens = result.scalars().all()

        if not monitored_tokens:
            logger.info("No monitored tokens to update")
            return {"updated": 0, "alerts_triggered": 0, "removed": 0}

        logger.info(f"Found {len(monitored_tokens)} monitored tokens (including alerted)")

        # Update prices and check for alerts
        updated_count = 0
        alerts_triggered = 0
        removed_count = 0
        removed_by_market_cap = 0
        removed_by_liquidity = 0
        import time

        async with self.db_manager.get_session() as session:
            for token in monitored_tokens:
                try:
                    # Fetch detailed data from AVE API
                    # ä½¿ç”¨ä»£å¸çš„ chain å­—æ®µï¼ˆbsc æˆ– solanaï¼‰
                    chain = getattr(token, 'chain', 'bsc')  # å…¼å®¹æ—§æ•°æ®ï¼Œé»˜è®¤ bsc
                    pair_data = ave_api_service.get_pair_detail_parsed(
                        pair_address=token.pair_address,
                        chain=chain
                    )

                    if not pair_data or not pair_data.get('current_price_usd'):
                        logger.warning(f"No price data for {token.token_symbol}")
                        time.sleep(delay)
                        continue

                    current_price = pair_data['current_price_usd']

                    # Update current price
                    token.current_price_usd = current_price
                    token.last_update_timestamp = datetime.utcnow()

                    # Update historical ATH (all-time high from blockchain)
                    if pair_data.get('price_ath_usd'):
                        old_ath = token.price_ath_usd
                        token.price_ath_usd = pair_data['price_ath_usd']
                        # å¦‚æœåˆ›æ–°é«˜ï¼Œè®°å½•æ—¥å¿—
                        if old_ath and token.price_ath_usd > old_ath:
                            logger.info(f"{token.token_symbol} new ATH: ${token.price_ath_usd} (was ${old_ath})")

                    # åŒæ­¥ peak_price_usd = price_ath_usdï¼ˆä¿æŒå­—æ®µä¸€è‡´æ€§ï¼‰
                    if token.price_ath_usd:
                        token.peak_price_usd = token.price_ath_usd
                        token.peak_timestamp = datetime.utcnow()

                    if pair_data.get('amm'):
                        token.amm = pair_data['amm']

                    if pair_data.get('current_tvl'):
                        token.current_tvl = pair_data['current_tvl']

                    if pair_data.get('current_market_cap'):
                        token.current_market_cap = pair_data['current_market_cap']

                    # Price changes
                    for timeframe in ['1m', '5m', '15m', '30m', '1h', '4h', '24h']:
                        field = f'price_change_{timeframe}'
                        if pair_data.get(field) is not None:
                            setattr(token, field, pair_data[field])

                    # Volumes
                    for timeframe in ['1m', '5m', '15m', '30m', '1h', '4h', '24h']:
                        field = f'volume_{timeframe}'
                        if pair_data.get(field) is not None:
                            setattr(token, field, pair_data[field])

                    # Transaction counts
                    for timeframe in ['1m', '5m', '15m', '30m', '1h', '4h', '24h']:
                        field = f'tx_count_{timeframe}'
                        if pair_data.get(field) is not None:
                            setattr(token, field, pair_data[field])

                    # Buy/sell data
                    if pair_data.get('buys_24h') is not None:
                        token.buys_24h = pair_data['buys_24h']
                    if pair_data.get('sells_24h') is not None:
                        token.sells_24h = pair_data['sells_24h']

                    # Traders
                    if pair_data.get('makers_24h') is not None:
                        token.makers_24h = pair_data['makers_24h']
                    if pair_data.get('buyers_24h') is not None:
                        token.buyers_24h = pair_data['buyers_24h']
                    if pair_data.get('sellers_24h') is not None:
                        token.sellers_24h = pair_data['sellers_24h']

                    # 24h price range
                    if pair_data.get('price_24h_high') is not None:
                        token.price_24h_high = pair_data['price_24h_high']
                    if pair_data.get('price_24h_low') is not None:
                        token.price_24h_low = pair_data['price_24h_low']
                    if pair_data.get('open_price_24h') is not None:
                        token.open_price_24h = pair_data['open_price_24h']

                    # LP info
                    if pair_data.get('lp_holders') is not None:
                        token.lp_holders = pair_data['lp_holders']
                    if pair_data.get('lp_locked_percent') is not None:
                        token.lp_locked_percent = pair_data['lp_locked_percent']
                    if pair_data.get('lp_lock_platform'):
                        token.lp_lock_platform = pair_data['lp_lock_platform']

                    # Early trading metrics
                    if pair_data.get('rusher_tx_count') is not None:
                        token.rusher_tx_count = pair_data['rusher_tx_count']
                    if pair_data.get('sniper_tx_count') is not None:
                        token.sniper_tx_count = pair_data['sniper_tx_count']

                    session.add(token)
                    updated_count += 1
                    # ä¸å†æ‰“å°æ¯ä¸ªä»£å¸çš„æˆåŠŸæ›´æ–°ï¼Œæœ€åæ±‡æ€»

                    # Check for price drop alert (using simple price data dict for compatibility)
                    price_data_dict = {
                        'price_usd': float(current_price),
                        'market_cap': float(pair_data.get('current_market_cap', 0)) if pair_data.get('current_market_cap') else None,
                        'liquidity': float(pair_data.get('current_tvl', 0)) if pair_data.get('current_tvl') else None,
                        'volume_24h': float(pair_data.get('volume_24h', 0)) if pair_data.get('volume_24h') else None,
                    }
                    if await self._check_and_trigger_alert(session, token, price_data_dict):
                        alerts_triggered += 1

                    # ä½¿ç”¨é€šç”¨æ–¹æ³•æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ é™¤
                    should_remove, removal_reason, removal_threshold = self._check_and_remove_by_thresholds(
                        token, min_market_cap, min_liquidity
                    )

                    if should_remove:
                        # æ ‡è®°åˆ é™¤
                        token.permanently_deleted = 1
                        token.removal_reason = removal_reason
                        token.removal_threshold_value = removal_threshold
                        token.deleted_at = datetime.utcnow()
                        removed_count += 1

                        # ç»Ÿè®¡åˆ†ç±»
                        if removal_reason == "low_market_cap":
                            removed_by_market_cap += 1
                        elif removal_reason == "low_liquidity":
                            removed_by_liquidity += 1

                        # æ—¥å¿—
                        logger.warning(
                            f"ğŸ—‘ï¸ Auto-removed {token.token_symbol}: {removal_reason} "
                            f"(value: {removal_threshold:.2f}, threshold: "
                            f"{min_market_cap if removal_reason == 'low_market_cap' else min_liquidity:.2f})"
                        )

                    # Delay to avoid rate limiting
                    time.sleep(delay)

                except Exception as e:
                    logger.error(f"Error updating {token.token_symbol}: {e}")
                    time.sleep(delay)
                    continue

            await session.commit()

        # æ±‡æ€»æˆåŠŸæ—¥å¿—
        if updated_count > 0:
            logger.info(
                f"âœ… æˆåŠŸæ›´æ–° {updated_count}/{len(monitored_tokens)} ä¸ªç›‘æ§ä»£å¸, "
                f"è§¦å‘æŠ¥è­¦ {alerts_triggered} æ¬¡"
            )
            if removed_count > 0:
                logger.info(
                    f"ğŸ—‘ï¸ è‡ªåŠ¨åˆ é™¤ {removed_count} ä¸ªä»£å¸ "
                    f"(å¸‚å€¼: {removed_by_market_cap}, æµåŠ¨æ€§: {removed_by_liquidity})"
                )
        else:
            logger.info(f"æœªæ›´æ–°ä»»ä½•ä»£å¸ (æ€»å…± {len(monitored_tokens)} ä¸ª)")

        return {
            "updated": updated_count,
            "alerts_triggered": alerts_triggered,
            "total_monitored": len(monitored_tokens),
            "removed": removed_count,
            "removed_by_market_cap": removed_by_market_cap,
            "removed_by_liquidity": removed_by_liquidity
        }

    def _check_and_remove_by_thresholds(
        self,
        token,
        min_market_cap: Optional[float],
        min_liquidity: Optional[float]
    ) -> tuple[bool, Optional[str], Optional[float]]:
        """
        æ£€æŸ¥ä»£å¸æ˜¯å¦åº”è¯¥è¢«åˆ é™¤ï¼ˆé€šç”¨ç­›é€‰é€»è¾‘ï¼‰

        Args:
            token: MonitoredToken æˆ– PotentialToken å¯¹è±¡
            min_market_cap: æœ€å°å¸‚å€¼é˜ˆå€¼ï¼ˆç¾å…ƒï¼‰
            min_liquidity: æœ€å°æµåŠ¨æ€§é˜ˆå€¼ï¼ˆç¾å…ƒï¼‰

        Returns:
            (should_remove, removal_reason, removal_threshold_value)
        """
        should_remove = False
        removal_reason = None
        removal_threshold = None

        # æ£€æŸ¥å¸‚å€¼é˜ˆå€¼
        if min_market_cap is not None and token.current_market_cap is not None:
            if float(token.current_market_cap) < min_market_cap:
                should_remove = True
                removal_reason = "low_market_cap"
                removal_threshold = float(token.current_market_cap)

        # æ£€æŸ¥æµåŠ¨æ€§é˜ˆå€¼ï¼ˆåªæœ‰åœ¨æœªè¢«å¸‚å€¼ç­›æ‰çš„æƒ…å†µä¸‹æ‰æ£€æŸ¥ï¼‰
        if not should_remove and min_liquidity is not None and token.current_tvl is not None:
            if float(token.current_tvl) < min_liquidity:
                should_remove = True
                removal_reason = "low_liquidity"
                removal_threshold = float(token.current_tvl)

        return should_remove, removal_reason, removal_threshold

    async def _check_and_trigger_alert(
        self,
        session: AsyncSession,
        token: MonitoredToken,
        price_data: Dict[str, Any]
    ) -> bool:
        """
        Check if alert should be triggered and create alert record.

        è®¡ç®—é€»è¾‘ï¼šä»å†å²æœ€é«˜ä»·(ATH)è®¡ç®—è·Œå¹…ï¼Œæ”¯æŒå¤šçº§é˜ˆå€¼æŠ¥è­¦

        å¤šçº§é˜ˆå€¼è®¾è®¡ï¼š
        - ä½¿ç”¨token.alert_thresholdsè‡ªå®šä¹‰é˜ˆå€¼åˆ—è¡¨ï¼ˆé»˜è®¤ [70, 80, 90]ï¼‰
        - æ¯ä¸ªä»£å¸å¯ä»¥æœ‰è‡ªå·±çš„é˜ˆå€¼åˆ—è¡¨
        - æ¯ä¸ªé˜ˆå€¼åªæŠ¥è­¦ä¸€æ¬¡ï¼Œé¿å…é‡å¤

        Args:
            session: Database session
            token: Monitored token instance
            price_data: Current price data

        Returns:
            True if alert was triggered
        """
        current_price = token.current_price_usd
        ath_price = token.price_ath_usd or token.peak_price_usd  # ä¼˜å…ˆä½¿ç”¨å†å²ATH
        entry_price = token.entry_price_usd

        # Calculate drop from ATH (å†å²æœ€é«˜ç‚¹)
        drop_from_ath = ((ath_price - current_price) / ath_price) * 100
        drop_from_entry = ((entry_price - current_price) / entry_price) * 100

        # ä½¿ç”¨è‡ªå®šä¹‰é˜ˆå€¼åˆ—è¡¨ï¼ˆæ¯ä¸ªä»£å¸æœ‰è‡ªå·±çš„é˜ˆå€¼é…ç½®ï¼‰
        thresholds = token.alert_thresholds if token.alert_thresholds else [70, 80, 90]
        # è½¬æ¢ä¸ºfloatåˆ—è¡¨ï¼ˆJSONBå¯èƒ½è¿”å›å…¶ä»–ç±»å‹ï¼‰
        thresholds = [float(t) for t in thresholds]

        # æ‰¾å‡ºå½“å‰è·Œå¹…è¾¾åˆ°çš„æœ€é«˜é˜ˆå€¼
        triggered_threshold = None
        for threshold in sorted(thresholds, reverse=True):
            if drop_from_ath >= threshold:
                triggered_threshold = threshold
                break

        # å¦‚æœæ²¡æœ‰è¾¾åˆ°ä»»ä½•é˜ˆå€¼ï¼Œä¸æŠ¥è­¦
        if triggered_threshold is None:
            return False

        # æŸ¥è¯¢è¯¥ä»£å¸æ‰€æœ‰å†å²æŠ¥è­¦ï¼Œæ£€æŸ¥è¿™ä¸ªé˜ˆå€¼æ˜¯å¦å·²ç»æŠ¥è­¦è¿‡
        all_alerts = await session.execute(
            select(PriceAlert).where(
                PriceAlert.monitored_token_id == token.id
            ).order_by(desc(PriceAlert.triggered_at))
        )
        existing_alerts = all_alerts.scalars().all()

        # æ£€æŸ¥æ˜¯å¦å·²ç»åœ¨è¿™ä¸ªé˜ˆå€¼çº§åˆ«æŠ¥è­¦è¿‡
        # é€»è¾‘ï¼šå¯¹äºæ¯æ¡å†å²æŠ¥è­¦ï¼Œè®¡ç®—å®ƒå±äºå“ªä¸ªé˜ˆå€¼çº§åˆ«
        for existing_alert in existing_alerts:
            existing_drop = float(existing_alert.drop_from_peak_percent)

            # è®¡ç®—è¯¥å†å²æŠ¥è­¦åº”è¯¥å±äºå“ªä¸ªé˜ˆå€¼çº§åˆ«
            existing_threshold = None
            for threshold in sorted(thresholds, reverse=True):
                if existing_drop >= threshold:
                    existing_threshold = threshold
                    break

            # å¦‚æœå†å²æŠ¥è­¦çš„é˜ˆå€¼çº§åˆ«ä¸å½“å‰è¦è§¦å‘çš„é˜ˆå€¼ç›¸åŒï¼Œåˆ™ä¸é‡å¤æŠ¥è­¦
            if existing_threshold == triggered_threshold:
                return False

        # Determine severity (åŸºäºATHè·Œå¹…)
        severity = "low"
        if drop_from_ath >= 70:
            severity = "critical"
        elif drop_from_ath >= 50:
            severity = "high"
        elif drop_from_ath >= 30:
            severity = "medium"

        # Create alert
        alert = PriceAlert(
            monitored_token_id=token.id,
            alert_type="price_drop",
            triggered_at=datetime.utcnow(),
            trigger_price_usd=current_price,
            peak_price_usd=ath_price,  # å­˜å‚¨ATHè€Œä¸æ˜¯ç›‘æ§æœŸé—´å³°å€¼
            entry_price_usd=entry_price,
            drop_from_peak_percent=Decimal(str(drop_from_ath)),  # ä»ATHçš„è·Œå¹…
            drop_from_entry_percent=Decimal(str(drop_from_entry)),
            market_cap=Decimal(str(price_data['market_cap'])) if price_data.get('market_cap') else None,
            liquidity_usd=Decimal(str(price_data['liquidity'])) if price_data.get('liquidity') else None,
            volume_24h=Decimal(str(price_data['volume_24h'])) if price_data.get('volume_24h') else None,
            message=f"{token.token_symbol} dropped {drop_from_ath:.2f}% from ATH ${ath_price} (threshold: {triggered_threshold}%)",
            severity=severity,
            acknowledged=0
        )

        session.add(alert)

        # Update token status to alerted (ä½†ä¼šç»§ç»­ç›‘æ§)
        token.status = "alerted"

        logger.warning(
            f"ğŸš¨ ALERT [{severity.upper()}]: {token.token_symbol} dropped {drop_from_ath:.2f}% from ATH! "
            f"ATH: ${ath_price}, Current: ${current_price}, Threshold: {triggered_threshold}%"
        )

        return True

    async def get_monitored_tokens(
        self,
        limit: int = 100,
        status: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """
        Get list of monitored tokens with optional status filter.

        Args:
            limit: Maximum number of tokens to return
            status: Filter by status (active/alerted/stopped), None returns all

        Returns:
            List of monitored token data
        """
        await self._ensure_db()

        async with self.db_manager.get_session() as session:
            # é»˜è®¤æ’é™¤å·²åˆ é™¤å’Œå½»åº•åˆ é™¤çš„ä»£å¸
            query = select(MonitoredToken).where(
                MonitoredToken.deleted_at.is_(None),
                MonitoredToken.permanently_deleted == 0
            )

            # Apply status filter if provided
            if status:
                query = query.where(MonitoredToken.status == status)

            query = query.order_by(desc(MonitoredToken.entry_timestamp)).limit(limit)

            result = await session.execute(query)
            tokens = result.scalars().all()

            return self._format_token_list(tokens)

    async def get_active_monitored_tokens(self, limit: int = 100) -> List[Dict[str, Any]]:
        """
        Get list of active monitored tokens.

        Args:
            limit: Maximum number of tokens to return

        Returns:
            List of monitored token data
        """
        await self._ensure_db()

        async with self.db_manager.get_session() as session:
            result = await session.execute(
                select(MonitoredToken)
                .where(MonitoredToken.status == "active")
                .order_by(desc(MonitoredToken.entry_timestamp))
                .limit(limit)
            )
            tokens = result.scalars().all()

            return self._format_token_list(tokens)

    async def get_alerts(
        self,
        limit: int = 50,
        acknowledged: Optional[bool] = None,
        severity: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """
        Get price alerts.

        Args:
            limit: Maximum number of alerts to return
            acknowledged: Filter by acknowledged status (None = all)
            severity: Filter by severity level

        Returns:
            List of alert data
        """
        await self._ensure_db()

        async with self.db_manager.get_session() as session:
            query = select(PriceAlert).join(MonitoredToken)

            # Apply filters
            conditions = []
            if acknowledged is not None:
                conditions.append(PriceAlert.acknowledged == (1 if acknowledged else 0))
            if severity:
                conditions.append(PriceAlert.severity == severity)

            if conditions:
                query = query.where(and_(*conditions))

            query = query.order_by(desc(PriceAlert.triggered_at)).limit(limit)

            result = await session.execute(query)
            alerts = result.scalars().all()

            # Fetch related monitored tokens
            alert_data = []
            for alert in alerts:
                token = await session.get(MonitoredToken, alert.monitored_token_id)
                alert_data.append({
                    "id": alert.id,
                    "token_symbol": token.token_symbol if token else "UNKNOWN",
                    "token_address": token.token_address if token else None,
                    "alert_type": alert.alert_type,
                    "triggered_at": alert.triggered_at.isoformat(),
                    "trigger_price_usd": float(alert.trigger_price_usd),
                    "peak_price_usd": float(alert.peak_price_usd),
                    "entry_price_usd": float(alert.entry_price_usd),
                    "drop_from_peak_percent": float(alert.drop_from_peak_percent),
                    "drop_from_entry_percent": float(alert.drop_from_entry_percent),
                    "message": alert.message,
                    "severity": alert.severity,
                    "acknowledged": bool(alert.acknowledged),
                })

            return alert_data

    # ==================== æ½œåŠ›å¸ç§ç›¸å…³æ–¹æ³• ====================

    async def scrape_and_save_to_potential(
        self,
        count: int = 100,
        top_n: int = 10,
        headless: bool = False
    ) -> Dict[str, Any]:
        """
        çˆ¬å–Topæ¶¨å¹…ä»£å¸å¹¶ä¿å­˜åˆ°æ½œåŠ›å¸ç§è¡¨

        æ›´æ–°ç­–ç•¥ï¼š
        - å¦‚æœä»£å¸ä¸å­˜åœ¨ï¼šåˆ›å»ºæ–°è®°å½•
        - å¦‚æœä»£å¸å·²å­˜åœ¨ï¼š
          - æ–°æ¶¨å¹… > åŸæ¶¨å¹…ï¼šæ›´æ–°æ‰€æœ‰çˆ¬å–å­—æ®µï¼ˆä»·æ ¼ã€æ¶¨å¹…ã€å¸‚å€¼ç­‰ï¼‰
          - æ–°æ¶¨å¹… <= åŸæ¶¨å¹…ï¼šè·³è¿‡ï¼Œä¿ç•™åŸè®°å½•çš„æœ€é«˜æ¶¨å¹…

        Args:
            count: çˆ¬å–ä»£å¸æ•°é‡
            top_n: ç­›é€‰å‰Nå
            headless: æ˜¯å¦ä½¿ç”¨æ— å¤´æµè§ˆå™¨

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸ {scraped, top_gainers, saved, skipped}
        """
        logger.info("\n" + "="*60)
        logger.info("ã€çˆ¬å–æ½œåŠ›å¸ç§ã€‘ä¿å­˜åˆ° potential_tokens è¡¨")
        logger.info("="*60)

        # æ­¥éª¤1: çˆ¬å–å’Œç­›é€‰
        scrape_result = self.scrape_and_filter_top_gainers(
            count=count,
            top_n=top_n,
            headless=headless
        )

        top_gainers = scrape_result["top_gainers"]
        if not top_gainers:
            return {
                "scraped": scrape_result["scraped_count"],
                "top_gainers": 0,
                "saved": 0,
                "skipped": 0,
                "filter_stats": scrape_result.get("filter_stats", {})
            }

        # æ­¥éª¤2: ä¿å­˜åˆ° potential_tokens è¡¨
        await self._ensure_db()

        added_count = 0
        skipped_count = 0

        async with self.db_manager.get_session() as session:
            for token_data in top_gainers:
                try:
                    # ä» DexScreener æ•°æ®ç»“æ„ä¸­æå–å­—æ®µ
                    base_token = token_data.get('baseToken', {})
                    token_address = base_token.get('address', '').lower()
                    token_symbol = base_token.get('symbol', 'UNKNOWN')
                    token_name = base_token.get('name', 'Unknown')

                    pair_address = token_data.get('pairAddress', '')
                    dex_id = token_data.get('dexId', 'unknown')

                    price_usd = float(token_data.get('priceUsd', 0))
                    price_change_24h = float(token_data.get('priceChange', {}).get('h24', 0))

                    market_cap = float(token_data.get('fdv', 0) or token_data.get('marketCap', 0) or 0)
                    liquidity_usd = float(token_data.get('liquidity', {}).get('usd', 0) or 0)
                    volume_24h = float(token_data.get('volume', {}).get('h24', 0) or 0)

                    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼ˆä½¿ç”¨ pair_addressï¼‰
                    result = await session.execute(
                        select(PotentialToken).where(
                            PotentialToken.pair_address == pair_address
                        )
                    )
                    existing = result.scalar_one_or_none()

                    if existing:
                        # ä»£å¸å·²å­˜åœ¨ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
                        old_change = existing.price_change_24h_at_scrape or 0

                        # å¦‚æœæ–°æ¶¨å¹…æ›´é«˜ï¼Œæ›´æ–°çˆ¬å–æ•°æ®
                        if price_change_24h > old_change:
                            existing.scraped_price_usd = price_usd
                            existing.scraped_timestamp = datetime.utcnow()
                            existing.market_cap_at_scrape = market_cap if market_cap > 0 else None
                            existing.liquidity_at_scrape = liquidity_usd if liquidity_usd > 0 else None
                            existing.volume_24h_at_scrape = volume_24h if volume_24h > 0 else None
                            existing.price_change_24h_at_scrape = price_change_24h

                            await session.flush()
                            logger.info(
                                f"ğŸ”„ Updated {token_symbol}: æ¶¨å¹…ä» {old_change:.1f}% â†’ {price_change_24h:.1f}% "
                                f"(+{price_change_24h - old_change:.1f}%)"
                            )
                            added_count += 1
                        else:
                            # æ¶¨å¹…æœªæé«˜ï¼Œè·³è¿‡æ›´æ–°
                            logger.info(
                                f"â­ï¸  {token_symbol} æ¶¨å¹…æœªæé«˜ "
                                f"(å½“å‰: {price_change_24h:.1f}%, æœ€é«˜: {old_change:.1f}%)"
                            )
                            skipped_count += 1
                        continue

                    # åˆ›å»ºæ–°çš„æ½œåŠ›å¸ç§è®°å½•
                    potential_token = PotentialToken(
                        token_address=token_address,
                        token_symbol=token_symbol,
                        token_name=token_name,
                        dex_id=dex_id,
                        pair_address=pair_address,
                        amm=None,  # é¡µé¢æ•°æ®æš‚æ—¶æ²¡æœ‰ AMM å­—æ®µ
                        scraped_price_usd=price_usd,
                        scraped_timestamp=datetime.utcnow(),
                        market_cap_at_scrape=market_cap if market_cap > 0 else None,
                        liquidity_at_scrape=liquidity_usd if liquidity_usd > 0 else None,
                        volume_24h_at_scrape=volume_24h if volume_24h > 0 else None,
                        price_change_24h_at_scrape=price_change_24h,
                        is_added_to_monitoring=0
                    )
                    session.add(potential_token)
                    await session.flush()

                    logger.info(f"âœ… Added {token_symbol} to potential_tokens (+{price_change_24h:.1f}%)")
                    added_count += 1

                except Exception as e:
                    logger.error(f"Error adding {token_data.get('baseToken', {}).get('symbol', 'UNKNOWN')}: {e}")
                    skipped_count += 1

        logger.info(
            f"\nâœ… Saved to potential_tokens: {added_count} added/updated, {skipped_count} skipped"
        )

        return {
            "scraped": scrape_result["scraped_count"],
            "filtered": scrape_result["filtered_count"],
            "top_gainers": len(top_gainers),
            "saved": added_count,  # åŒ…å«æ–°å¢å’Œæ›´æ–°çš„æ•°é‡
            "skipped": skipped_count,
            "filter_stats": scrape_result.get("filter_stats", {})
        }

    async def get_potential_tokens(
        self,
        limit: int = 100,
        only_not_added: bool = False
    ) -> List[Dict[str, Any]]:
        """
        è·å–æ½œåŠ›å¸ç§åˆ—è¡¨

        Args:
            limit: è¿”å›æ•°é‡
            only_not_added: ä»…è¿”å›æœªæ·»åŠ åˆ°ç›‘æ§çš„

        Returns:
            æ½œåŠ›å¸ç§åˆ—è¡¨
        """
        await self._ensure_db()

        async with self.db_manager.get_session() as session:
            # é»˜è®¤æ’é™¤å·²åˆ é™¤å’Œå½»åº•åˆ é™¤çš„ä»£å¸
            query = select(PotentialToken).where(
                PotentialToken.deleted_at.is_(None),
                PotentialToken.permanently_deleted == 0
            )

            if only_not_added:
                query = query.where(PotentialToken.is_added_to_monitoring == 0)

            query = query.order_by(desc(PotentialToken.scraped_timestamp)).limit(limit)

            result = await session.execute(query)
            tokens = result.scalars().all()

            return self._format_potential_token_list(tokens)

    def _format_potential_token_list(self, tokens: list) -> List[Dict[str, Any]]:
        """Format a list of PotentialToken objects to dictionaries."""
        return [
            {
                # åŸºç¡€ä¿¡æ¯
                "id": token.id,
                "token_address": token.token_address,
                "token_symbol": token.token_symbol,
                "token_name": token.token_name,
                "chain": token.chain,
                "dex_id": token.dex_id,
                "pair_address": token.pair_address,
                "amm": token.amm,
                "dex_type": token.dex_type,

                # çˆ¬å–æ—¶çš„ä»·æ ¼å’Œå¸‚åœºæ•°æ®
                "scraped_price_usd": float(token.scraped_price_usd),
                "scraped_timestamp": token.scraped_timestamp.isoformat() if token.scraped_timestamp else None,
                "market_cap_at_scrape": float(token.market_cap_at_scrape) if token.market_cap_at_scrape else None,
                "liquidity_at_scrape": float(token.liquidity_at_scrape) if token.liquidity_at_scrape else None,
                "volume_24h_at_scrape": float(token.volume_24h_at_scrape) if token.volume_24h_at_scrape else None,
                "price_change_24h_at_scrape": float(token.price_change_24h_at_scrape) if token.price_change_24h_at_scrape else None,

                # å½“å‰æ•°æ®ï¼ˆAVE APIæ›´æ–°åï¼‰
                "current_price_usd": float(token.current_price_usd) if token.current_price_usd else None,
                "price_ath_usd": float(token.price_ath_usd) if token.price_ath_usd else None,
                "current_tvl": float(token.current_tvl) if token.current_tvl else None,
                "current_market_cap": float(token.current_market_cap) if token.current_market_cap else None,

                # æ—¶é—´æˆ³
                "token_created_at": token.token_created_at.isoformat() if token.token_created_at else None,
                "first_trade_at": token.first_trade_at.isoformat() if token.first_trade_at else None,
                "last_ave_update": token.last_ave_update.isoformat() if token.last_ave_update else None,

                # ä»·æ ¼å˜åŒ–
                "price_change_1m": float(token.price_change_1m) if token.price_change_1m else None,
                "price_change_5m": float(token.price_change_5m) if token.price_change_5m else None,
                "price_change_15m": float(token.price_change_15m) if token.price_change_15m else None,
                "price_change_30m": float(token.price_change_30m) if token.price_change_30m else None,
                "price_change_1h": float(token.price_change_1h) if token.price_change_1h else None,
                "price_change_4h": float(token.price_change_4h) if token.price_change_4h else None,
                "price_change_24h": float(token.price_change_24h) if token.price_change_24h else None,

                # äº¤æ˜“é‡
                "volume_1m": float(token.volume_1m) if token.volume_1m else None,
                "volume_5m": float(token.volume_5m) if token.volume_5m else None,
                "volume_15m": float(token.volume_15m) if token.volume_15m else None,
                "volume_30m": float(token.volume_30m) if token.volume_30m else None,
                "volume_1h": float(token.volume_1h) if token.volume_1h else None,
                "volume_4h": float(token.volume_4h) if token.volume_4h else None,
                "volume_24h": float(token.volume_24h) if token.volume_24h else None,

                # äº¤æ˜“æ¬¡æ•°
                "tx_count_1m": token.tx_count_1m,
                "tx_count_5m": token.tx_count_5m,
                "tx_count_15m": token.tx_count_15m,
                "tx_count_30m": token.tx_count_30m,
                "tx_count_1h": token.tx_count_1h,
                "tx_count_4h": token.tx_count_4h,
                "tx_count_24h": token.tx_count_24h,

                # ä¹°å–æ•°æ®
                "buys_24h": token.buys_24h,
                "sells_24h": token.sells_24h,

                # äº¤æ˜“è€…æ•°æ®
                "makers_24h": token.makers_24h,
                "buyers_24h": token.buyers_24h,
                "sellers_24h": token.sellers_24h,

                # 24å°æ—¶ä»·æ ¼èŒƒå›´
                "price_24h_high": float(token.price_24h_high) if token.price_24h_high else None,
                "price_24h_low": float(token.price_24h_low) if token.price_24h_low else None,
                "open_price_24h": float(token.open_price_24h) if token.open_price_24h else None,

                # LPä¿¡æ¯
                "lp_holders": token.lp_holders,
                "lp_locked_percent": float(token.lp_locked_percent) if token.lp_locked_percent else None,
                "lp_lock_platform": token.lp_lock_platform,

                # å®‰å…¨æŒ‡æ ‡
                "rusher_tx_count": token.rusher_tx_count,
                "sniper_tx_count": token.sniper_tx_count,

                # Tokenåˆ›å»ºä¿¡æ¯
                "creation_block_number": token.creation_block_number,
                "creation_tx_hash": token.creation_tx_hash,

                # çŠ¶æ€
                "is_added_to_monitoring": bool(token.is_added_to_monitoring),
                "added_to_monitoring_at": token.added_to_monitoring_at.isoformat() if token.added_to_monitoring_at else None,
            }
            for token in tokens
        ]

    async def add_potential_to_monitoring(
        self,
        potential_token_id: str,
        drop_threshold: float = 20.0
    ) -> Dict[str, Any]:
        """
        å°†æ½œåŠ›å¸ç§æ·»åŠ åˆ°ç›‘æ§è¡¨

        Args:
            potential_token_id: æ½œåŠ›å¸ç§ID
            drop_threshold: è·Œå¹…æŠ¥è­¦é˜ˆå€¼

        Returns:
            æ“ä½œç»“æœ
        """
        await self._ensure_db()

        async with self.db_manager.get_session() as session:
            # è·å–æ½œåŠ›å¸ç§
            potential_token = await session.get(PotentialToken, potential_token_id)
            if not potential_token:
                raise ValueError(f"Potential token not found: {potential_token_id}")

            # æ£€æŸ¥æ˜¯å¦å·²æ·»åŠ åˆ°ç›‘æ§
            if potential_token.is_added_to_monitoring:
                raise ValueError(f"Token {potential_token.token_symbol} already added to monitoring")

            # æ£€æŸ¥ç›‘æ§è¡¨ä¸­æ˜¯å¦å·²å­˜åœ¨è¯¥ pair
            result = await session.execute(
                select(MonitoredToken).where(
                    MonitoredToken.pair_address == potential_token.pair_address
                )
            )
            existing_monitored = result.scalar_one_or_none()
            if existing_monitored:
                raise ValueError(f"Token {potential_token.token_symbol} already in monitored_tokens")

            # åˆ›å»ºç›‘æ§è®°å½•ï¼ˆä½¿ç”¨å½“å‰ä»·æ ¼æˆ–çˆ¬å–ä»·æ ¼ï¼‰
            entry_price = float(potential_token.current_price_usd or potential_token.scraped_price_usd)

            monitored_token = MonitoredToken(
                token_address=potential_token.token_address,
                token_symbol=potential_token.token_symbol,
                token_name=potential_token.token_name,
                chain=getattr(potential_token, 'chain', 'bsc'),  # å¤åˆ¶ chain å­—æ®µ
                dex_id=potential_token.dex_id,
                pair_address=potential_token.pair_address,
                amm=potential_token.amm,
                dex_type=getattr(potential_token, 'dex_type', None),  # å¤åˆ¶ dex_type å­—æ®µ
                entry_price_usd=entry_price,
                peak_price_usd=entry_price,  # åˆå§‹å³°å€¼ = å…¥åœºä»·
                entry_timestamp=datetime.utcnow(),
                peak_timestamp=datetime.utcnow(),
                market_cap_at_entry=float(potential_token.current_market_cap or potential_token.market_cap_at_scrape or 0),
                liquidity_at_entry=float(potential_token.liquidity_at_scrape or 0),
                volume_24h_at_entry=float(potential_token.volume_24h or potential_token.volume_24h_at_scrape or 0),
                price_change_24h_at_entry=float(potential_token.price_change_24h or potential_token.price_change_24h_at_scrape or 0),
                status="active",
                drop_threshold_percent=drop_threshold
            )

            # å¤åˆ¶AVE APIæ•°æ®
            monitored_token.current_price_usd = potential_token.current_price_usd
            monitored_token.price_ath_usd = potential_token.price_ath_usd
            monitored_token.current_tvl = potential_token.current_tvl
            monitored_token.current_market_cap = potential_token.current_market_cap

            # å¤åˆ¶å¤šæ—¶é—´æ®µæ•°æ®
            for timeframe in ['1m', '5m', '15m', '30m', '1h', '4h', '24h']:
                for prefix in ['price_change', 'volume', 'tx_count']:
                    field = f'{prefix}_{timeframe}'
                    value = getattr(potential_token, field, None)
                    if value is not None:
                        setattr(monitored_token, field, value)

            # å¤åˆ¶å…¶ä»–å­—æ®µ
            for field in ['buys_24h', 'sells_24h', 'makers_24h', 'buyers_24h', 'sellers_24h',
                          'price_24h_high', 'price_24h_low', 'open_price_24h',
                          'token_created_at', 'first_trade_at', 'creation_block_number', 'creation_tx_hash',
                          'lp_holders', 'lp_locked_percent', 'lp_lock_platform',
                          'rusher_tx_count', 'sniper_tx_count']:
                value = getattr(potential_token, field, None)
                if value is not None:
                    setattr(monitored_token, field, value)

            session.add(monitored_token)

            # æ ‡è®°æ½œåŠ›å¸ç§å·²æ·»åŠ 
            potential_token.is_added_to_monitoring = 1
            potential_token.added_to_monitoring_at = datetime.utcnow()

            await session.flush()

            logger.info(f"âœ… Added {potential_token.token_symbol} to monitoring (entry: ${entry_price:.8f})")

            return {
                "success": True,
                "token_symbol": potential_token.token_symbol,
                "monitored_token_id": monitored_token.id,
                "entry_price_usd": entry_price
            }

    async def delete_potential_token(self, potential_token_id: str) -> Dict[str, Any]:
        """
        è½¯åˆ é™¤æ½œåŠ›å¸ç§ï¼ˆè®¾ç½®deleted_atè€Œä¸æ˜¯çœŸæ­£åˆ é™¤ï¼‰

        Args:
            potential_token_id: æ½œåŠ›å¸ç§ID

        Returns:
            æ“ä½œç»“æœ
        """
        await self._ensure_db()

        async with self.db_manager.get_session() as session:
            potential_token = await session.get(PotentialToken, potential_token_id)
            if not potential_token:
                raise ValueError(f"Potential token not found: {potential_token_id}")

            if potential_token.deleted_at is not None:
                raise ValueError(f"Token already deleted: {potential_token.token_symbol}")

            token_symbol = potential_token.token_symbol

            # è½¯åˆ é™¤ï¼šè®¾ç½® deleted_at æ—¶é—´æˆ³
            potential_token.deleted_at = datetime.utcnow()
            await session.flush()

            logger.info(f"ğŸ—‘ï¸  Soft deleted potential token: {token_symbol}")

            return {
                "success": True,
                "token_symbol": token_symbol,
                "deleted_at": potential_token.deleted_at.isoformat()
            }

    async def get_deleted_potential_tokens(self, limit: int = 100) -> List[Dict[str, Any]]:
        """
        è·å–å·²åˆ é™¤çš„æ½œåŠ›ä»£å¸åˆ—è¡¨

        Args:
            limit: æœ€å¤§è¿”å›æ•°é‡

        Returns:
            å·²åˆ é™¤çš„æ½œåŠ›ä»£å¸åˆ—è¡¨
        """
        await self._ensure_db()

        async with self.db_manager.get_session() as session:
            # åªè¿”å›è½¯åˆ é™¤ï¼ˆdeleted_at ä¸ä¸ºç©ºï¼‰ï¼Œä½†æœªå½»åº•åˆ é™¤çš„ä»£å¸
            query = select(PotentialToken).where(
                PotentialToken.deleted_at.isnot(None),
                PotentialToken.permanently_deleted == 0
            ).order_by(desc(PotentialToken.deleted_at)).limit(limit)

            result = await session.execute(query)
            tokens = result.scalars().all()

            return self._format_potential_token_list(tokens)

    async def restore_potential_token(self, potential_token_id: str) -> Dict[str, Any]:
        """
        æ¢å¤å·²åˆ é™¤çš„æ½œåŠ›ä»£å¸

        Args:
            potential_token_id: æ½œåŠ›ä»£å¸ID

        Returns:
            æ“ä½œç»“æœ
        """
        await self._ensure_db()

        async with self.db_manager.get_session() as session:
            potential_token = await session.get(PotentialToken, potential_token_id)
            if not potential_token:
                raise ValueError(f"Potential token not found: {potential_token_id}")

            if potential_token.deleted_at is None:
                raise ValueError(f"Token is not deleted: {potential_token.token_symbol}")

            token_symbol = potential_token.token_symbol

            # æ¢å¤ï¼šæ¸…é™¤ deleted_at æ—¶é—´æˆ³
            potential_token.deleted_at = None
            await session.flush()

            logger.info(f"â™»ï¸  Restored potential token: {token_symbol}")

            return {
                "success": True,
                "token_symbol": token_symbol
            }

    async def delete_monitored_token(self, monitored_token_id: str) -> Dict[str, Any]:
        """
        è½¯åˆ é™¤ç›‘æ§ä»£å¸

        Args:
            monitored_token_id: ç›‘æ§ä»£å¸ID

        Returns:
            æ“ä½œç»“æœ
        """
        await self._ensure_db()

        async with self.db_manager.get_session() as session:
            monitored_token = await session.get(MonitoredToken, monitored_token_id)
            if not monitored_token:
                raise ValueError(f"Monitored token not found: {monitored_token_id}")

            if monitored_token.deleted_at is not None:
                raise ValueError(f"Token already deleted: {monitored_token.token_symbol}")

            token_symbol = monitored_token.token_symbol

            # è½¯åˆ é™¤ï¼šè®¾ç½® deleted_at æ—¶é—´æˆ³
            monitored_token.deleted_at = datetime.utcnow()
            await session.flush()

            logger.info(f"ğŸ—‘ï¸  Soft deleted monitored token: {token_symbol}")

            return {
                "success": True,
                "token_symbol": token_symbol,
                "deleted_at": monitored_token.deleted_at.isoformat()
            }

    async def get_deleted_monitored_tokens(self, limit: int = 100) -> List[Dict[str, Any]]:
        """
        è·å–å·²åˆ é™¤çš„ç›‘æ§ä»£å¸åˆ—è¡¨

        Args:
            limit: æœ€å¤§è¿”å›æ•°é‡

        Returns:
            å·²åˆ é™¤çš„ç›‘æ§ä»£å¸åˆ—è¡¨
        """
        await self._ensure_db()

        async with self.db_manager.get_session() as session:
            # åªè¿”å›è½¯åˆ é™¤ï¼ˆdeleted_at ä¸ä¸ºç©ºï¼‰ï¼Œä½†æœªå½»åº•åˆ é™¤çš„ä»£å¸
            query = select(MonitoredToken).where(
                MonitoredToken.deleted_at.isnot(None),
                MonitoredToken.permanently_deleted == 0
            ).order_by(desc(MonitoredToken.deleted_at)).limit(limit)

            result = await session.execute(query)
            tokens = result.scalars().all()

            return self._format_token_list(tokens)

    async def restore_monitored_token(self, monitored_token_id: str) -> Dict[str, Any]:
        """
        æ¢å¤å·²åˆ é™¤çš„ç›‘æ§ä»£å¸

        Args:
            monitored_token_id: ç›‘æ§ä»£å¸ID

        Returns:
            æ“ä½œç»“æœ
        """
        await self._ensure_db()

        async with self.db_manager.get_session() as session:
            monitored_token = await session.get(MonitoredToken, monitored_token_id)
            if not monitored_token:
                raise ValueError(f"Monitored token not found: {monitored_token_id}")

            if monitored_token.deleted_at is None:
                raise ValueError(f"Token is not deleted: {monitored_token.token_symbol}")

            token_symbol = monitored_token.token_symbol

            # æ¢å¤ï¼šæ¸…é™¤ deleted_at æ—¶é—´æˆ³
            monitored_token.deleted_at = None
            await session.flush()

            logger.info(f"â™»ï¸  Restored monitored token: {token_symbol}")

            return {
                "success": True,
                "token_symbol": token_symbol
            }

    async def update_potential_tokens_data(
        self,
        delay: float = 0.3,
        min_update_interval_minutes: int = 3
    ) -> Dict[str, Any]:
        """
        æ›´æ–°æ‰€æœ‰æ½œåŠ›å¸ç§çš„AVE APIæ•°æ®

        Args:
            delay: APIè°ƒç”¨é—´éš”ï¼ˆç§’ï¼‰
            min_update_interval_minutes: æœ€å°æ›´æ–°é—´éš”ï¼ˆåˆ†é’Ÿï¼‰ï¼Œé¿å…é¢‘ç¹è°ƒç”¨

        Returns:
            æ›´æ–°ç»Ÿè®¡
        """
        await self._ensure_db()

        # åŠ è½½ç›‘æ§é…ç½®ï¼ˆç”¨äºç­›é€‰é˜ˆå€¼ï¼‰
        monitor_config = None
        async with self.db_manager.get_session() as session:
            config_result = await session.execute(
                select(MonitorConfig).limit(1)
            )
            monitor_config = config_result.scalar_one_or_none()

        min_market_cap = None
        min_liquidity = None
        if monitor_config:
            min_market_cap = float(monitor_config.min_monitor_market_cap) if monitor_config.min_monitor_market_cap else None
            min_liquidity = float(monitor_config.min_monitor_liquidity) if monitor_config.min_monitor_liquidity else None
            if min_market_cap or min_liquidity:
                logger.info(f"æ½œåŠ›ä»£å¸ç­›é€‰é˜ˆå€¼: å¸‚å€¼ >= {min_market_cap}, æµåŠ¨æ€§ >= {min_liquidity}")

        # æ£€æŸ¥æ˜¯å¦éœ€è¦è·³è¿‡æœ¬æ¬¡æ›´æ–°ï¼ˆé¿å…é‡å¤è°ƒç”¨ï¼‰
        async with self.db_manager.get_session() as session:
            # æŸ¥è¯¢æœ€è¿‘æ›´æ–°çš„æ½œåŠ›ä»£å¸
            result = await session.execute(
                select(PotentialToken)
                .where(PotentialToken.last_ave_update.isnot(None))
                .order_by(desc(PotentialToken.last_ave_update))
                .limit(1)
            )
            latest_token = result.scalar_one_or_none()

            if latest_token and latest_token.last_ave_update:
                minutes_since_update = (datetime.utcnow() - latest_token.last_ave_update).total_seconds() / 60
                if minutes_since_update < min_update_interval_minutes:
                    logger.info(
                        f"Skipping update: last update was {minutes_since_update:.1f} minutes ago "
                        f"(min interval: {min_update_interval_minutes} minutes)"
                    )
                    return {"updated": 0, "failed": 0, "skipped": True}

        logger.info("Updating potential tokens with AVE API data...")

        updated_count = 0
        failed_count = 0
        removed_count = 0
        removed_by_market_cap = 0
        removed_by_liquidity = 0
        import time

        # åœ¨åŒä¸€ä¸ª session ä¸­æŸ¥è¯¢å’Œæ›´æ–°
        async with self.db_manager.get_session() as session:
            # è·å–æ‰€æœ‰æœªæ·»åŠ åˆ°ç›‘æ§çš„æ½œåŠ›å¸ç§ï¼ˆæ’é™¤å·²åˆ é™¤ï¼‰
            result = await session.execute(
                select(PotentialToken).where(
                    PotentialToken.is_added_to_monitoring == 0,
                    PotentialToken.deleted_at.is_(None),
                    PotentialToken.permanently_deleted == 0
                )
            )
            potential_tokens = result.scalars().all()

            if not potential_tokens:
                logger.info("No potential tokens to update")
                return {"updated": 0, "failed": 0}

            logger.info(f"Found {len(potential_tokens)} potential tokens to update")

            for token in potential_tokens:
                try:
                    # è·å–AVE APIæ•°æ®
                    # ä½¿ç”¨ä»£å¸çš„ chain å­—æ®µï¼ˆbsc æˆ– solanaï¼‰
                    chain = getattr(token, 'chain', 'bsc')  # å…¼å®¹æ—§æ•°æ®ï¼Œé»˜è®¤ bsc
                    pair_data = ave_api_service.get_pair_detail_parsed(
                        pair_address=token.pair_address,
                        chain=chain
                    )

                    if not pair_data:
                        logger.warning(f"No AVE data for {token.token_symbol}")
                        failed_count += 1
                        time.sleep(delay)
                        continue

                    # æ›´æ–°æ‰€æœ‰AVE APIå­—æ®µï¼ˆå’Œ MonitoredToken ä¸€æ ·çš„é€»è¾‘ï¼‰
                    # æ›´æ–°çœŸå®çš„ token åˆçº¦åœ°å€ï¼ˆä¿®æ­£çˆ¬è™«æ—¶ä½¿ç”¨ pair_address çš„é—®é¢˜ï¼‰
                    if pair_data.get('token_address'):
                        token.token_address = pair_data['token_address']

                    if pair_data.get('current_price_usd'):
                        token.current_price_usd = pair_data['current_price_usd']

                    if pair_data.get('price_ath_usd'):
                        token.price_ath_usd = pair_data['price_ath_usd']

                    if pair_data.get('amm'):
                        token.amm = pair_data['amm']

                    if pair_data.get('current_tvl'):
                        token.current_tvl = pair_data['current_tvl']

                    if pair_data.get('current_market_cap'):
                        token.current_market_cap = pair_data['current_market_cap']

                    # ä»·æ ¼å˜åŒ–
                    for timeframe in ['1m', '5m', '15m', '30m', '1h', '4h', '24h']:
                        field = f'price_change_{timeframe}'
                        if pair_data.get(field) is not None:
                            setattr(token, field, pair_data[field])

                    # äº¤æ˜“é‡
                    for timeframe in ['1m', '5m', '15m', '30m', '1h', '4h', '24h']:
                        field = f'volume_{timeframe}'
                        if pair_data.get(field) is not None:
                            setattr(token, field, pair_data[field])

                    # äº¤æ˜“æ¬¡æ•°
                    for timeframe in ['1m', '5m', '15m', '30m', '1h', '4h', '24h']:
                        field = f'tx_count_{timeframe}'
                        if pair_data.get(field) is not None:
                            setattr(token, field, pair_data[field])

                    # ä¹°å–æ•°æ®
                    if pair_data.get('buys_24h') is not None:
                        token.buys_24h = pair_data['buys_24h']
                    if pair_data.get('sells_24h') is not None:
                        token.sells_24h = pair_data['sells_24h']

                    # äº¤æ˜“è€…æ•°æ®
                    for field in ['makers_24h', 'buyers_24h', 'sellers_24h']:
                        if pair_data.get(field) is not None:
                            setattr(token, field, pair_data[field])

                    # ä»·æ ¼èŒƒå›´
                    for field in ['price_24h_high', 'price_24h_low', 'open_price_24h']:
                        if pair_data.get(field) is not None:
                            setattr(token, field, pair_data[field])

                    # Tokenåˆ›å»ºä¿¡æ¯
                    if pair_data.get('token_created_at'):
                        token.token_created_at = pair_data['token_created_at']
                    if pair_data.get('first_trade_at'):
                        token.first_trade_at = pair_data['first_trade_at']
                    if pair_data.get('creation_block_number'):
                        token.creation_block_number = pair_data['creation_block_number']
                    if pair_data.get('creation_tx_hash'):
                        token.creation_tx_hash = pair_data['creation_tx_hash']

                    # LPä¿¡æ¯
                    if pair_data.get('lp_holders') is not None:
                        token.lp_holders = pair_data['lp_holders']
                    if pair_data.get('lp_locked_percent') is not None:
                        token.lp_locked_percent = pair_data['lp_locked_percent']
                    if pair_data.get('lp_lock_platform'):
                        token.lp_lock_platform = pair_data['lp_lock_platform']

                    # å®‰å…¨æŒ‡æ ‡
                    if pair_data.get('rusher_tx_count') is not None:
                        token.rusher_tx_count = pair_data['rusher_tx_count']
                    if pair_data.get('sniper_tx_count') is not None:
                        token.sniper_tx_count = pair_data['sniper_tx_count']

                    token.last_ave_update = datetime.utcnow()
                    await session.flush()

                    # ä½¿ç”¨é€šç”¨æ–¹æ³•æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ é™¤
                    should_remove, removal_reason, removal_threshold = self._check_and_remove_by_thresholds(
                        token, min_market_cap, min_liquidity
                    )

                    if should_remove:
                        # æ ‡è®°åˆ é™¤
                        token.permanently_deleted = 1
                        token.removal_reason = removal_reason
                        token.removal_threshold_value = removal_threshold
                        token.deleted_at = datetime.utcnow()
                        removed_count += 1

                        # ç»Ÿè®¡åˆ†ç±»
                        if removal_reason == "low_market_cap":
                            removed_by_market_cap += 1
                        elif removal_reason == "low_liquidity":
                            removed_by_liquidity += 1

                        # æ—¥å¿—
                        logger.warning(
                            f"ğŸ—‘ï¸ Auto-removed potential token {token.token_symbol}: {removal_reason} "
                            f"(value: {removal_threshold:.2f}, threshold: "
                            f"{min_market_cap if removal_reason == 'low_market_cap' else min_liquidity:.2f})"
                        )

                    # ä¸å†æ‰“å°æ¯ä¸ªä»£å¸çš„æˆåŠŸæ›´æ–°ï¼Œæœ€åæ±‡æ€»
                    updated_count += 1

                    time.sleep(delay)

                except Exception as e:
                    logger.error(f"Error updating {token.token_symbol}: {e}")
                    failed_count += 1
                    time.sleep(delay)

            # æ±‡æ€»æˆåŠŸæ—¥å¿—
            if updated_count > 0:
                logger.info(
                    f"âœ… æˆåŠŸæ›´æ–° {updated_count}/{len(potential_tokens)} ä¸ªæ½œåŠ›ä»£å¸ AVE æ•°æ®"
                    + (f", {failed_count} ä¸ªå¤±è´¥" if failed_count > 0 else "")
                )
                if removed_count > 0:
                    logger.info(
                        f"ğŸ—‘ï¸ è‡ªåŠ¨åˆ é™¤ {removed_count} ä¸ªæ½œåŠ›ä»£å¸ "
                        f"(å¸‚å€¼: {removed_by_market_cap}, æµåŠ¨æ€§: {removed_by_liquidity})"
                    )
            else:
                logger.info(f"æœªæ›´æ–°ä»»ä½•æ½œåŠ›ä»£å¸ (æ€»å…± {len(potential_tokens)} ä¸ª, {failed_count} ä¸ªå¤±è´¥)")

            return {
                "updated": updated_count,
                "failed": failed_count,
                "removed": removed_count,
                "removed_by_market_cap": removed_by_market_cap,
                "removed_by_liquidity": removed_by_liquidity
            }

    async def get_monitor_config(self) -> Optional[Dict[str, Any]]:
        """
        è·å–ç›‘æ§é…ç½®

        Returns:
            é…ç½®å­—å…¸ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å›None
        """
        await self._ensure_db()

        async with self.db_manager.get_session() as session:
            try:
                query = select(MonitorConfig).limit(1)
                result = await session.execute(query)
                config = result.scalar_one_or_none()

                if not config:
                    logger.warning("æœªæ‰¾åˆ°ç›‘æ§é…ç½®ï¼Œä½¿ç”¨é»˜è®¤å€¼")
                    return {
                        "enabled": True,
                        "update_interval_minutes": 5,
                        "min_monitor_market_cap": None,
                        "min_monitor_liquidity": None,
                        "max_retry_count": 3,
                        "batch_size": 10
                    }

                return {
                    "id": config.id,
                    "enabled": bool(config.enabled),
                    "update_interval_minutes": config.update_interval_minutes,
                    "min_monitor_market_cap": float(config.min_monitor_market_cap) if config.min_monitor_market_cap else None,
                    "min_monitor_liquidity": float(config.min_monitor_liquidity) if config.min_monitor_liquidity else None,
                    "max_retry_count": config.max_retry_count,
                    "batch_size": config.batch_size,
                    "description": config.description
                }

            except Exception as e:
                logger.error(f"è·å–ç›‘æ§é…ç½®å¤±è´¥: {e}")
                return None

    async def get_scraper_config(self) -> Optional[Dict[str, Any]]:
        """
        è·å–çˆ¬è™«é…ç½®

        Returns:
            é…ç½®å­—å…¸ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å›None
        """
        await self._ensure_db()

        async with self.db_manager.get_session() as session:
            try:
                query = select(ScraperConfig).where(ScraperConfig.enabled == 1).limit(1)
                result = await session.execute(query)
                config = result.scalar_one_or_none()

                if not config:
                    logger.warning("æœªæ‰¾åˆ°å¯ç”¨çš„çˆ¬è™«é…ç½®")
                    return None

                return {
                    "id": config.id,
                    "top_n_per_chain": config.top_n_per_chain,
                    "count_per_chain": config.count_per_chain,
                    "scrape_interval_min": config.scrape_interval_min,
                    "scrape_interval_max": config.scrape_interval_max,
                    "enabled_chains": config.enabled_chains,  # JSONB field, already parsed as list
                    "use_undetected_chrome": bool(config.use_undetected_chrome),
                    "enabled": bool(config.enabled),
                    "description": config.description,
                    # ç­›é€‰æ¡ä»¶
                    "min_market_cap": float(config.min_market_cap) if config.min_market_cap else None,
                    "min_liquidity": float(config.min_liquidity) if config.min_liquidity else None,
                    "max_token_age_days": config.max_token_age_days
                }

            except Exception as e:
                logger.error(f"è·å–çˆ¬è™«é…ç½®å¤±è´¥: {e}")
                return None

    async def add_monitoring_by_pair(
        self,
        pair_address: str,
        chain: str,
        drop_threshold: float = 20.0,
        alert_thresholds: Optional[List[float]] = None
    ) -> dict:
        """
        é€šè¿‡ pair åœ°å€æ‰‹åŠ¨æ·»åŠ ç›‘æ§ä»£å¸

        æµç¨‹ï¼š
        1. è°ƒç”¨ AVE API è·å– pair è¯¦ç»†ä¿¡æ¯
        2. æå–ä»£å¸ä¿¡æ¯å¹¶åˆ›å»ºç›‘æ§è®°å½•
        """
        from src.storage.models import MonitoredToken
        from sqlalchemy import select
        import uuid

        # 1. è°ƒç”¨ AVE API è·å– pair è¯¦æƒ…
        pair_data = ave_api_service.get_pair_detail_parsed(pair_address, chain)
        if not pair_data:
            raise ValueError(
                f"æ— æ³•æ‰¾åˆ° pair: {pair_address} (é“¾: {chain})ã€‚"
                f"å¯èƒ½åŸå› ï¼š1) pair åœ°å€ä¸æ­£ç¡® 2) AVE API æš‚æœªæ”¶å½•è¯¥ pair 3) é“¾åç§°é”™è¯¯"
            )

        # 2. æå–ä»£å¸ä¿¡æ¯
        token_address = pair_data.get('token_address') or pair_address
        token_symbol = pair_data.get('token_symbol') or 'Unknown'
        token_name = pair_data.get('token_name') or 'Unknown'

        # å®‰å…¨è·å–ä»·æ ¼ï¼ˆå¯èƒ½ä¸º Noneï¼‰
        price_value = pair_data.get('current_price_usd')
        if price_value is None:
            raise ValueError(
                f"æ— æ³•è·å– pair {pair_address} çš„ä»·æ ¼ä¿¡æ¯ã€‚"
                f"pair æ•°æ®å¯èƒ½ä¸å®Œæ•´æˆ– AVE API æ•°æ®å¼‚å¸¸ã€‚"
            )

        current_price = float(price_value)
        if current_price <= 0:
            raise ValueError(f"è·å–åˆ°çš„ä»£å¸ä»·æ ¼æ— æ•ˆ: {current_price}")

        await self._ensure_db()

        # 3. æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›‘æ§
        async with self.db_manager.get_session() as session:
            result = await session.execute(
                select(MonitoredToken).where(
                    MonitoredToken.pair_address == pair_address,
                    MonitoredToken.chain == chain,
                    MonitoredToken.permanently_deleted == 0
                )
            )
            existing = result.scalar_one_or_none()

            if existing:
                raise ValueError(f"è¯¥ä»£å¸å·²åœ¨ç›‘æ§åˆ—è¡¨ä¸­ï¼ˆ{existing.token_symbol}ï¼‰")

            # 4. åˆ›å»ºç›‘æ§è®°å½•
            # è¾…åŠ©å‡½æ•°ï¼šå®‰å…¨è½¬æ¢Decimal
            def safe_decimal(value):
                if value is None:
                    return None
                try:
                    return float(value) if isinstance(value, (int, float, Decimal)) else float(value)
                except:
                    return None

            monitored_token = MonitoredToken(
                id=str(uuid.uuid4()),
                token_address=token_address,
                token_symbol=token_symbol,
                token_name=token_name,
                chain=chain,
                dex_id='ave',
                pair_address=pair_address,
                amm=pair_data.get('amm'),
                dex_type=pair_data.get('dex_type'),
                entry_price_usd=current_price,
                peak_price_usd=current_price,
                current_price_usd=current_price,
                drop_threshold_percent=drop_threshold,
                alert_thresholds=alert_thresholds or [70, 80, 90],
                status='active',

                # å†å²æœ€é«˜ä»·ï¼ˆATHï¼‰
                price_ath_usd=safe_decimal(pair_data.get('price_ath_usd')),

                # å¸‚åœºæ•°æ®
                current_tvl=safe_decimal(pair_data.get('current_tvl')),
                current_market_cap=safe_decimal(pair_data.get('current_market_cap')),

                # ä»·æ ¼å˜åŒ–ï¼ˆå¤šæ—¶é—´æ®µï¼‰
                price_change_1m=safe_decimal(pair_data.get('price_change_1m')),
                price_change_5m=safe_decimal(pair_data.get('price_change_5m')),
                price_change_15m=safe_decimal(pair_data.get('price_change_15m')),
                price_change_30m=safe_decimal(pair_data.get('price_change_30m')),
                price_change_1h=safe_decimal(pair_data.get('price_change_1h')),
                price_change_4h=safe_decimal(pair_data.get('price_change_4h')),
                price_change_24h=safe_decimal(pair_data.get('price_change_24h')),

                # äº¤æ˜“é‡ï¼ˆå¤šæ—¶é—´æ®µï¼‰
                volume_1m=safe_decimal(pair_data.get('volume_1m')),
                volume_5m=safe_decimal(pair_data.get('volume_5m')),
                volume_15m=safe_decimal(pair_data.get('volume_15m')),
                volume_30m=safe_decimal(pair_data.get('volume_30m')),
                volume_1h=safe_decimal(pair_data.get('volume_1h')),
                volume_4h=safe_decimal(pair_data.get('volume_4h')),
                volume_24h=safe_decimal(pair_data.get('volume_24h')),

                # äº¤æ˜“æ¬¡æ•°ï¼ˆå¤šæ—¶é—´æ®µï¼‰
                tx_count_1m=pair_data.get('tx_count_1m'),
                tx_count_5m=pair_data.get('tx_count_5m'),
                tx_count_15m=pair_data.get('tx_count_15m'),
                tx_count_30m=pair_data.get('tx_count_30m'),
                tx_count_1h=pair_data.get('tx_count_1h'),
                tx_count_4h=pair_data.get('tx_count_4h'),
                tx_count_24h=pair_data.get('tx_count_24h'),

                # ä¹°å–æ•°æ®
                buys_24h=pair_data.get('buys_24h'),
                sells_24h=pair_data.get('sells_24h'),

                # äº¤æ˜“è€…æ•°æ®
                makers_24h=pair_data.get('makers_24h'),
                buyers_24h=pair_data.get('buyers_24h'),
                sellers_24h=pair_data.get('sellers_24h'),

                # 24å°æ—¶ä»·æ ¼èŒƒå›´
                price_24h_high=safe_decimal(pair_data.get('price_24h_high')),
                price_24h_low=safe_decimal(pair_data.get('price_24h_low')),
                open_price_24h=safe_decimal(pair_data.get('open_price_24h')),

                # LPä¿¡æ¯
                lp_holders=pair_data.get('lp_holders'),
                lp_locked_percent=safe_decimal(pair_data.get('lp_locked_percent')),
                lp_lock_platform=pair_data.get('lp_lock_platform'),

                # å®‰å…¨æŒ‡æ ‡
                rusher_tx_count=pair_data.get('rusher_tx_count'),
                sniper_tx_count=pair_data.get('sniper_tx_count'),

                # Tokenåˆ›å»ºä¿¡æ¯
                token_created_at=pair_data.get('token_created_at'),
                first_trade_at=pair_data.get('first_trade_at'),
                creation_block_number=pair_data.get('creation_block_number'),
                creation_tx_hash=pair_data.get('creation_tx_hash'),
            )

            session.add(monitored_token)
            await session.commit()
            await session.refresh(monitored_token)

            logger.info(f"âœ… å·²æ·»åŠ åˆ°ç›‘æ§: {token_symbol} (pair: {pair_address[:10]}...)")

            return {
                "success": True,
                "message": f"å·²æ·»åŠ  {token_symbol} åˆ°ç›‘æ§åˆ—è¡¨",
                "token_id": monitored_token.id,
                "token_symbol": token_symbol,
                "entry_price": current_price
            }

    async def permanently_delete_monitored_token(self, token_id: str) -> dict:
        """
        å½»åº•åˆ é™¤ç›‘æ§ä»£å¸ï¼ˆè®¾ç½® permanently_deleted=1ï¼‰
        """
        from src.storage.models import MonitoredToken
        from sqlalchemy import select
        from datetime import datetime

        await self._ensure_db()

        async with self.db_manager.get_session() as session:
            result = await session.execute(
                select(MonitoredToken).where(MonitoredToken.id == token_id)
            )
            token = result.scalar_one_or_none()

            if not token:
                raise ValueError(f"æœªæ‰¾åˆ°IDä¸º {token_id} çš„ç›‘æ§ä»£å¸")

            token.permanently_deleted = 1
            token.deleted_at = datetime.utcnow()
            await session.commit()

            logger.info(f"ğŸ—‘ï¸ å½»åº•åˆ é™¤ç›‘æ§ä»£å¸: {token.token_symbol}")

            return {
                "success": True,
                "message": f"å·²å½»åº•åˆ é™¤ {token.token_symbol}"
            }

    async def permanently_delete_potential_token(self, token_id: str) -> dict:
        """
        å½»åº•åˆ é™¤æ½œåŠ›ä»£å¸ï¼ˆè®¾ç½® permanently_deleted=1ï¼‰
        """
        from src.storage.models import PotentialToken
        from sqlalchemy import select
        from datetime import datetime

        await self._ensure_db()

        async with self.db_manager.get_session() as session:
            result = await session.execute(
                select(PotentialToken).where(PotentialToken.id == token_id)
            )
            token = result.scalar_one_or_none()

            if not token:
                raise ValueError(f"æœªæ‰¾åˆ°IDä¸º {token_id} çš„æ½œåŠ›ä»£å¸")

            token.permanently_deleted = 1
            token.deleted_at = datetime.utcnow()
            await session.commit()

            logger.info(f"ğŸ—‘ï¸ å½»åº•åˆ é™¤æ½œåŠ›ä»£å¸: {token.token_symbol}")

            return {
                "success": True,
                "message": f"å·²å½»åº•åˆ é™¤ {token.token_symbol}"
            }
